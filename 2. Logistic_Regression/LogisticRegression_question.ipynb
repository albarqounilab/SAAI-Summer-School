{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVcuRU2x0o4I"
   },
   "source": [
    "# Logistic Regression (~ 100 min)\n",
    "\n",
    "![img](https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/iris.png)\n",
    "\n",
    "In this notebook, we will explore and work with the Iris data set (which deals with different types of the Iris flower). The Iris flower data set, or Fisher's Iris data set, is a multivariate data set used and made famous by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. More information can be found at https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "\n",
    "Image source: Unwin, Antony and Kim Kleinman. \"The iris data set: In search of the source of virginica.\" Significance 18.6 (2021): 26-29.\n",
    "First, we need to import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdhvkD9aWBQf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "#Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtbzGsUM0o4K"
   },
   "source": [
    "ToDo (15 min):\n",
    "\n",
    "What is each library helpful for? (You can use Google to familiarise yourself with the libraries)\n",
    "Which of the lines above imports a package? Which imports a function?\n",
    "\n",
    "## Loading the Data (10 min)\n",
    "\n",
    "Use the load_iris function we imported in the previous step to load the data set (note that the function returns the data set, which must be stored in a variable).\n",
    "\n",
    "Create a Pandas dataframe object containing the features (the iris['data']) and the target values (iris[target]). Note that the column names correspond to the feature names (iris['feature_names]), and an additional column is needed for the target value.\n",
    "\n",
    "Familiarize yourself with the data set. How many different varieties of iris are in this data set?\n",
    "You can use pd.head() to print the first 5 records in the data set. \n",
    "You can use column_example.unique() to find all the unique elements in the example column of the Pandas data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11-KNhRsW2Mx"
   },
   "source": [
    "Load the data and determine the number of classes in the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlPw3O_XWbvU"
   },
   "outputs": [],
   "source": [
    "#ToDo: Load the dataset\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Explain the parameters of the initialising function\n",
    "dataset = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "#ToDo: Print the different possible target values\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Print the first 5 records of the dataset\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtMeYqXx0o4L"
   },
   "source": [
    "## Data Exploration (15 min)\n",
    "\n",
    "A table of numbers is not useful for interpreting dependencies between variables. Instead, let's plot the data and identify which features correlate with each other.\n",
    "\n",
    "We will use two types of plots: scatter plots and histograms. It is important to note that scatter plots show the relationship between two variables, while histograms show the distribution of a single variable. To learn more about these types of plots, please refer to the matplot library.\n",
    "To plot the correlation between our 14 features, we require a total of 196 plots. Please complete the code below, utilizing axs[COLUMN_INDEX, ROW_INDEX] to select a subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJDiPoOOXRyt"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(dataset.columns), len(dataset.columns), figsize=(25, 25))\n",
    "\n",
    "for i in range(len(dataset.columns)):\n",
    "    for j in range(len(dataset.columns)):\n",
    "        if i == j:\n",
    "            # We plot the histogram of column i on axis i, j\n",
    "            axs[i,j].hist(dataset[dataset.columns[i]])\n",
    "        else:\n",
    "            #ToDo: scatter plot the data points with column i as x and column j as y,\n",
    "            #with the color of the dot defined by target value\n",
    "            #Insert your own code here\n",
    "        axs[0,j].set_title(dataset.columns[j])\n",
    "    axs[i,0].set(ylabel=dataset.columns[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tewXBKic0o4L"
   },
   "source": [
    "To analyze our task and make future predictions, it's important to calculate the correlation between features and the target value, as well as between features themselves. A correlation matrix can be a helpful visualization tool.\n",
    "\n",
    "A correlation matrix ([https://www.displayr.com/what-is-a-correlation-matrix/](https://www.displayr.com/what-is-a-correlation-matrix/) is a table that displays correlation coefficients between variables. Each cell in the table shows the correlation between two variables. It is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.\n",
    "\n",
    "To represent the correlation between variables, a correlation coefficient is used in a correlation matrix.\n",
    "\n",
    "To compute the correlation matrix on your pandas dataframe, use a_pandas_dataframe.corr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-tHzbbwbhVV"
   },
   "outputs": [],
   "source": [
    "#ToDo compute the correlation matrix (into the variable correlation_matrix)\n",
    "#Insert your own code here\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "#ToDo explain what the negative and positive values represent\n",
    "#Optional : change the color scheme (see documentation of the function for help)\n",
    "sns.heatmap(data=correlation_matrix, annot=True, ax=ax) # annot = True to print the values inside the square\n",
    "plt.show()\n",
    "dataset.boxplot(by=\"target\", figsize=(5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6KWRlqS0o4L"
   },
   "source": [
    "## Training the model (15 min)\n",
    "\n",
    "In machine learning, model performance is typically evaluated using a test set, which consists of data that was not used during the training process. This ensures a more reliable evaluation.\n",
    "\n",
    "To achieve this, we randomly split the dataset into a training set and a test set, with the latter containing 25% of the data.\n",
    "\n",
    "Note that we have imported the function train_test_split from the sklearn module, submodel model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EnteaJU89NJ"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "X = dataset.iloc[:, [0,1,2, 3]].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "#ToDo split the dataset into train and test of 25% (into X_train, X_test, y_train, y_test) with random_state=0\n",
    "#Insert your own code here\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-Pnofc10o4L"
   },
   "source": [
    "To train the classifier, first create and initialize the classifier object, in this case, a LogisticRegression object. \n",
    "\n",
    "Then, use the classifier.fit() function to fit the classifier to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_w1a8Rk9k2F"
   },
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "#Initialising the classifier\n",
    "classifier = LogisticRegression(random_state = 0, solver='lbfgs', multi_class='auto')\n",
    "#ToDo: fit the classifier to the train set\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrN2Tmla0o4L"
   },
   "source": [
    "## Evaluation of the classifier (15 min)\n",
    "\n",
    "After fitting the model, we must evaluate its performance to determine if the method's complexity or amount of training data is sufficient.\n",
    "\n",
    "To complete this process, follow these steps:\n",
    "1. Use the trained model to predict the target values of the test set. You can do this by using `classifer.predict()`.\n",
    "2. Use the trained model to predict the probabilities for each target class on the test set. To obtain the prediction in the form of probabilities for each target class, we can utilize `classifier.predict_proba()`.\n",
    "3. Then, we can compare the resulting probabilities to the correct result and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC71-n2XBuSY"
   },
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Predict probabilities (into probs_y variable)\n",
    "#Insert your own code here\n",
    "\n",
    "probs_y = np.round(probs_y, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYHtJfesB7vm"
   },
   "outputs": [],
   "source": [
    "#We plot a table with target results, predicted class and probabilities for each class for every data point\n",
    "res = \"{:<10} | {:<10} | {:<10} | {:<13} | {:<5}\".format(\"y_test\", \"y_pred\", \"Setosa(%)\", \"versicolor(%)\", \"virginica(%)\\n\")\n",
    "res += \"-\"*65+\"\\n\"\n",
    "res += \"\\n\".join(\"{:<10} | {:<10} | {:<10} | {:<13} | {:<10}\".format(x, y, a, b, c) for x, y, a, b, c in zip(y_test, y_pred, probs_y[:,0], probs_y[:,1], probs_y[:,2]))\n",
    "res += \"\\n\"+\"-\"*65+\"\\n\"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E6lIUc90o4L"
   },
   "source": [
    "Our classifier's performance may vary across classes. For instance, some classes may be easily predicted while others are consistently misclassified.\n",
    "\n",
    "A confusion matrix enables us to visualize the distinction between each class.\n",
    "\n",
    "Each row represents the actual class, and each column represents the predicted class. The number at the intersection of row `r` and column `c` indicates how often our model predicts a point in class `r` as belonging to class `c`. The name of this matrix comes from its ability to clearly display how often our classifier confuses any two classes.\n",
    "\n",
    "To compute the confusion matrix, use the `confusion_matrix()` function imported from `sklearn.metrics`.\n",
    "To visualize the confusion matrix as a heatmap (a matrix of values where the color of each cell depends on the value), use the `heatmap()` function from the seaborn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAyrjfToCFV5"
   },
   "outputs": [],
   "source": [
    "#ToDo:  compute the confusion matrix on the test set into variable cm\n",
    "#Insert your own code here\n",
    "\n",
    "print(cm)\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "\n",
    "#Visualising the confusion matrix\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 10}, fmt='d',cmap=\"Blues\", ax = ax )\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cuR2IeS0o4M"
   },
   "source": [
    "# Logistic Regression on MNIST (20 min, extra task)\n",
    "\n",
    "\n",
    "The MNIST dataset (https://www.tensorflow.org/datasets/catalog/mnist) consists of handwritten digits ranging from 0 to 9.\n",
    "\n",
    "This dataset presents a challenge for classification due to the larger number of features per data point and possible classes, making it a closer representation of real-life scenarios.To download the dataset, data loaders will be utilized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23bbYjUu0o4M"
   },
   "outputs": [],
   "source": [
    "#Load MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "images, labels = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFFhczVS0o4M"
   },
   "outputs": [],
   "source": [
    "#ToDo: split into train and test set\n",
    "\n",
    "#ToDo split the dataset into train and test (into variables images_train, images_test, labels_train, labels_test )\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_A-YOFKX0o4M"
   },
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "plt.imshow(images[0].reshape(28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfZiGnlK0o4M"
   },
   "source": [
    "# MNIST Data exploration\n",
    "\n",
    "To better understand the dataset, we can visualise several images from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QxzquAL0o4M"
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the images in the batch, along with the true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx].reshape(28, 28)), cmap='gray')\n",
    "    ax.set_title(labels[idx])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii21JgxK0o4M"
   },
   "source": [
    "After gaining an understanding of the types of images present in the dataset, we should examine one image in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GY4R4dZS0o4M"
   },
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1].reshape(28, 28))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "\n",
    "width, height = img.shape\n",
    "thresh = img.max() / 2.5\n",
    "\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        # Round the values to two decimal places\n",
    "        val = round(img[x][y], 2) if img[x][y] != 0 else 0\n",
    "        ax.annotate(f'{val/255:.2f}', xy=(y, x), horizontalalignment='center', verticalalignment='center',\n",
    "                    color='white' if img[x][y] < thresh else 'black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UztEKIoL0o4M"
   },
   "source": [
    "To train on the newly loaded MNIST dataset, we can repeat the training process used for the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4ahyxyS0o4M"
   },
   "outputs": [],
   "source": [
    "#Fit a model to MNIST\n",
    "\n",
    "#ToDo: create and initialise the classifier\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: fit the classifier to the train set\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK5dDnxF0o4M"
   },
   "source": [
    "Let us evaluate the performance of our classifier. We expect the performance to be worse, as the problem is now more complex.\n",
    "This highlights the need for more advanced machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t95VPV5b0o4M"
   },
   "outputs": [],
   "source": [
    "#Evaluate the model on MNIST dataset\n",
    "\n",
    "#ToDo: Predict the Test set results (into y_pred)\n",
    "#Insert your own code here\n",
    "\n",
    "#ToDo: Predict probabilities (into probs_y)\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBXDLLiv0o4M"
   },
   "outputs": [],
   "source": [
    "#Visualise the input and prediction (same as input but predicted with percentages)\n",
    "# Plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    img = np.squeeze(np.array(images_test)[idx].reshape(28, 28))\n",
    "    true_label = labels_test[labels_test.keys()[idx]]\n",
    "\n",
    "    # Format the percentage with two decimal places\n",
    "    percentage = \"{:.2f}%\".format(probs_y[idx][int(y_pred[idx])] * 100)\n",
    "\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(\"{} (true {}), {}\".format(str(y_pred[idx]), str(true_label), percentage),\n",
    "                 color=(\"green\" if y_pred[idx] == true_label else \"red\"))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWh4dz9d0o4M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
