{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (90 min)\n",
    "\n",
    "Over the past 140 years, we’ve literally gone from making some temperature measurements by hand to using sophisticated satellite technology. Today’s temperature data come from many sources, including thousands of land weather stations, weather balloons, radar, ships and buoys, satellites, and volunteer weather watchers.\n",
    "\n",
    "It is important to make use of this increasingly accurate data to predict the future weather.\n",
    "\n",
    "In this session we will explore the rain in Australia dataset. This dataset contains daily weather observations from numerous Australian weather stations for approximately 10 years (22 data columns+target variable).\n",
    "\n",
    "The observations were gathered from a multitude of weather stations. You can access daily observations from http://www.bom.gov.au/climate/data.\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/NASAAus.jpeg\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://gpm.nasa.gov/category/keywords/imerg\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/rainAus.jpeg\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://www.thaipbsworld.com/heavy-rain-flash-flooding-batter-australias-east-coast/\n",
    "\n",
    "To accurately draw conclusions from the data, we must know and understand the input and target variables.\n",
    "\n",
    "<u>Examples of what columns (representing variables) the dataset has</u>:\n",
    "- \"Date\": The date of observation\n",
    "- \"Location\":The common name of the location of the weather station\n",
    "- \"Rainfall\":  The amount of rainfall recorded for the day in mm\n",
    "- \"Evaporation\" : The so-called Class A pan evaporation (mm) - how much water evaporates in the 24 hours to 9am\n",
    "- \"Sunshine\" : The number of hours of bright sunshine in the day.\n",
    "- \"WindGustDir\": The direction of the strongest wind gust in the 24 hours to midnight (W=West, E=East, NW=North-west and similar)\n",
    "- \"WindGustSpeed\" : The speed (km/h) of the strongest wind gust in the 24 hours to midnight\n",
    "- \"WindDir9am\" : Direction of the wind at 9am (W=West, E=East, NW=North-west and similar)\n",
    "- \"WindDir3pm\" : Direction of the wind at 3pm (W=West, E=East, NW=North-west and similar)\n",
    "- \"WindSpeed9am\": Wind speed (km/hr) averaged over 10 minutes prior to 9am\n",
    "- \"WindSpeed3pm\": Wind speed (km/hr) averaged over 10 minutes prior to 3am\n",
    "- \"MinTemp\": The minimum temperature for the day in degrees celsius\n",
    "- \"MaxTemp\": The maximum temperature for the day in degrees celsius\n",
    "- \"Humidity9am\": Humidity (percent) at 9am\n",
    "- \"Pressure9am\": Atmospheric pressure (hpa = hectoPascals or millibars) reduced to mean sea level at 9am\n",
    "- \"Humidity3am\": Humidity (percent) at 3pm\n",
    "- \"Pressure3pm\": Atmospheric pressure (hpa = hectoPascals or millibars) reduced to mean sea level at 3pm\n",
    "- \"Cloud9am\": Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n",
    "- \"Cloud3pm\": Fraction of sky obscured by cloud at 3pm. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n",
    "- \"Temp9am\": Temperature (degrees C) at 9am\n",
    "- \"Temp3pm\": Temperature (degrees C) at 3pm\n",
    "- \"RainToday\": 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n",
    "\n",
    "The <u>target</u> (called **RainTomorrow**) means: did it rain the next day? The possible values are \"Yes\" or \"No\", with yes meaning that that there was at least 1mm of rain or more.\n",
    "\n",
    "We will use **logistic regression model** to predict the RainTomorrow variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries (<5 min)\n",
    "\n",
    "Before we even start, we need to import the libraries with functions and data types/structures that we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#First we need to import the libraries with functions and data types/structures that we are going to use\n",
    "#ToDo: discuss if you are familiar with the libraries and what the purpose of each one is\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "#Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation (60 min)\n",
    "\n",
    "It is important to explore and preprocess the data before training an algorithm on any dataset.\n",
    "This contributes to efficiency and the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Importing Data (5 min)\n",
    "\n",
    "First we need to load the dataset that we will work with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#ToDo: load the data from csv file 'https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/weatherAUS.csv' into the variable data via the pandas read_csv function\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "\n",
    "#ToDo: look at a few entries to get a feeling for what the data is like using the sample function on the variable \n",
    "#data with the parameter 5 (that will show 5 random data points from the dataset) \n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Exploring Data (30 mins)\n",
    "\n",
    "The treatment of the dataset and especially the preparation depends on the qualities of the dataset.\n",
    "Let us explore the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration of data missingness (10 min)\n",
    "\n",
    "In the random data points we see NaN. How common is that? \n",
    "How we deal with these missing values depends on how many of them there are in out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: calculate how many NaN there are overall using the .isna() function to get a True/False array\n",
    "#(whether a value is missing or not) and summing up the True elements with .sum() function\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms (5 min)\n",
    "\n",
    "Histograms allow us to explore the range and distribution of values for each variable.\n",
    "To create a histogram, the value range is divided into \"bins\" (smaller ranges of values). The more \"bins\" we create, the more detailed our histogram.\n",
    "The histogram represents how many data points have a value in each bin for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: visualise histograms of some columns with calling a .hist() function from data into the variable hist\n",
    "#Try parameters bins=50 and figsize=(15, 15) [size of resulting figure]\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all columns were visualised. Why?\n",
    "The hist function only wok with numbers, so we will need to convert our strings to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new variable data_categorical where all string will be converted to numbers\n",
    "data_categorical = copy.deepcopy(data)\n",
    "\n",
    "data_categorical['Location']=data_categorical['Location'].astype('category').cat.codes\n",
    "data_categorical['RainToday']=data_categorical['RainToday'].astype('category').cat.codes\n",
    "data_categorical['WindDir9am']=data_categorical['WindDir9am'].astype('category').cat.codes\n",
    "data_categorical['WindDir3pm']=data_categorical['WindDir3pm'].astype('category').cat.codes\n",
    "data_categorical['WindGustDir']=data_categorical['WindGustDir'].astype('category').cat.codes\n",
    "data_categorical['RainTomorrow']=data_categorical['RainTomorrow'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (5 min)\n",
    "\n",
    "Correlation means - to which degree do 2 variables change together? \n",
    "Examples - temperature outside and how much water evaporates per given time are very connected.\n",
    "           amount of sun amd mood are connected\n",
    "           temperature in one country and dates of exams in another, however, are not connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: calculate the correlation using the .corr() function\n",
    "#for all the variables in the data_categorical with the 'RainTomorrow' target\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix (5 min)\n",
    "\n",
    "The correlation can be presented in a more visual way as a matrix (where each cell represents the correlation between the 2 variables). \n",
    "\n",
    "Let us take a look at our correlation matrix for this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: create the variable named correlation_matrix via the .corr() function of data_categorical\n",
    "#Round the values to 2 numbers after the decimal point for more efficient visualisation (add .round(2))\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "\n",
    "#ToDo visualise the correlation matrix as a heatmap (sns.heatmap function with variable correlation_matrix as \n",
    "#data and variable ax as ax, annot=True (to enable the numbers inside squares of heatmap) and cmap='coolwarm')\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cleaning Data (5 min)\n",
    "\n",
    "We see that not all columns are equally useful for predicting the probability of rain.\n",
    "Let's remove the columns that are not helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ToDo: Getting rid of the columns with variables which will not be used in our model \n",
    "#('Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Temp9am') using the drop() function\n",
    "#of the variable data with parameters axis=1 and implace=True\n",
    "\n",
    "#Insert your own code here\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Missing data imputation (10 mins)\n",
    "\n",
    "<u>Missing data (value) imputation</u> - replacing missing data with an estimated value based on other available information, allowing to keep all data points (especially important in small datasets) \n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/miss_val.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://towardsdatascience.com/how-to-handle-missing-data-b557c9e82fa0\n",
    "\n",
    "Every subject in a randomly chosen sample can be replaced by a new subject that is randomly chosen from the same source population as the original subject, without compromising the conclusions. Imputation techniques are also based on this basic principle of replacement. *[\"Review: A gentle introduction to imputation of missing values\", A. Rogier et al., 2006, Journal of Clinical Epidemiology 59 (2006) 1087e1091]*\n",
    "\n",
    "The right type of missing value imputation to choose depends on the type of missing data.\n",
    "\n",
    "<u>Types of missing data</u>:\n",
    "1. **Missing completely at random**\n",
    "- accidental breaking, deletion of data, corruption, contamination etc.\n",
    "- can be correctly imputed by simplest methods\n",
    "2. **Missing not at random**\n",
    "- reason for missingness ... is related to *unobserved* ... characteristics\n",
    "- almost impossible to correctly impute \n",
    "3. **Missing at random**\n",
    "- reason for missingness is based on other *observed* ... characteristics\n",
    "- missing data can ... be considered random conditional on these other patient characteristics that determined their missingness and that are available at the time of analysis \n",
    "- all simple techniques for handling missing data, i.e., complete and available case analyses, the indicator method and overall mean imputation, give biased results. \n",
    "\n",
    "The simple algorithms often either cannot run on datasets with missing data, or ignore the samples with missing data, so we will apply missing value imputation.\n",
    "\n",
    "Here we will use **mean value imputation** (for each variable, which is represented by a column in the dataset, the missing value will be replaced by the mean of that variable/column)\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/mean_imp_example.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://towardsdatascience.com/imputing-missing-data-with-simple-and-advanced-techniques-f5c7b157fb87\n",
    "\n",
    "Feel free to experiment and explore further!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ToDo: replace NaN values with mean values of each column with fillna() function:\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Converting Predictions to Binary for Logistic Regression (5 mins)\n",
    "\n",
    "The algorithm was programmed to deal with numbers, not strings, so we will need to replace strings with numerical values that represent the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: change that day and next days'predictions (strings \"Yes\" and \"No\" \n",
    "#in colunms RainToday and RainTomorrow of data) to 1 and 0:\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Excluding Tomorrow's Prediction from the Dataset (5 mins)\n",
    "\n",
    "It is crucial for training an algorithm to provide the target variable y and the dataset x (the variables from which we will predict our target). So far they have been mixed here, so let's separate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: separate the data[x_data] and the target variable[y]\n",
    "#use the values method on the data.RainTomorrow to create the y\n",
    "#use the drop function of the variable data with parameter axis=1 to drop the 'RainTomorrow' to create x_data\n",
    "\n",
    "#Insert your own code here\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Data Normalization (5 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Data normalization</u> - transforming the range of features to a common (usually standard) scale (often 0-1). \n",
    "\n",
    "**Motivation**: \n",
    "1. Normalized data enhances model performance and improves the accuracy of a model.\n",
    "- all features contribute equally\n",
    "- the trends and variations are more visible and easily identifiable\n",
    "2. Makes the training more stable.\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/norm.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://www.codecademy.com/article/normalization\n",
    "\n",
    "**Types of normalization:** <br>\n",
    "1)<u>Min-Max Scaling</u> <br>\n",
    "\n",
    "Min-max scaling is very often simply called ‘normalization.’ It transforms features to a specified range, typically between 0 and 1. The formula for min-max scaling is:<br>\n",
    "\n",
    "**X_normalized** = (x - min(x)) / (max(x) - min(x)) <br>\n",
    "\n",
    "That most often used technique works when:\n",
    "- The approximate upper and lower bounds of the dataset is known, and the dataset has few or no outliers\n",
    "- The data distribution is unknown or non-Gaussian, and the data is approximately uniformly distributed across the range\n",
    "\n",
    "2) <u>Z-score normalization (standardization)</u><br>\n",
    "\n",
    "Z-score normalization assumes a Gaussian (bell curve) distribution of the data and transforms features to have a mean (μ) of 0 and a standard deviation (σ) of 1. The formula for standardization is:\n",
    "\n",
    "**X_standardized** = x−μ / σ\n",
    "\n",
    "- This technique is particularly useful when dealing with algorithms that assume normally distributed data, such as many linear models. \n",
    "\n",
    "3)<u>Decimal scaling normalization</u>\n",
    "\n",
    "The objective of decimal scaling normalization is to scale the feature values by a power of 10, ensuring that the largest absolute value in each feature becomes less than 1. It is useful when the range of values in a dataset is known, but the range varies across features. The formula for decimal scaling normalization is:\n",
    "\n",
    "**X_decimal** = X / 10d\n",
    "\n",
    "Where X is the original feature value, and d is the smallest integer such that the largest absolute value in the feature becomes less than 1.\n",
    "\n",
    "- Decimal scaling normalization is advantageous when dealing with datasets where the absolute magnitude of values matters more than their specific scale.\n",
    "\n",
    "4)<u>Log scaling normalization</u>\n",
    "\n",
    "Log scaling normalization converts data into a logarithmic scale, by taking the log of each data point. It is particularly useful when dealing with data that spans several orders of magnitude. The formula for log scaling normalization is:\n",
    "\n",
    "**X_log** = log(X)\n",
    "\n",
    "- This normalization comes in handy with data that follows an exponential growth or decay pattern. It compresses the scale of the dataset, making it easier for models to capture patterns and relationships in the data. Population size over the years is a good example of a dataset where some features exhibit exponential growth. \n",
    "- Log scaling normalization can make these features more amenable to modeling.\n",
    "\n",
    "5)<u>Robust scaling normalization</u>\n",
    "\n",
    "Robust scaling normalization is useful when working with datasets that have outliers. It uses the median and interquartile range (IQR) instead of the mean and standard deviation to handle outliers. The formula for robust scaling is:\n",
    "\n",
    "**Xrobust** = X – median/ IQR #ToDo: formula\n",
    "\n",
    "- Since robust scaling is resilient to the influence of outliers, this makes it suitable for datasets with skewed or anomalous values.\n",
    "\n",
    "We saw (from the histograms) that the range of values in features is very different, so let's perform normalization to deal with this challenge.\n",
    "**We will use the min-max scaling here**, but feel free to try different and more complex techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: perform min-max normalization on the rain dataset and write the result into variable x:\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "x.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Dividing Dataset for Training and Testing the Model (5 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of your model fairly, some part of the data needs to remain unseen and unknown to the model. Similar to exam preparation questions and actual exams, the data on which the model trains needs to be similar and representative of the data the model will have to work on, but not identical.\n",
    "\n",
    "<u> Representative sample </u>: A representative sample is defined as a small quantity or a subset of something larger. It represents the same properties and proportions like that of a larger population.\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/repr.jpg\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://www.questionpro.com/blog/representative-sample/\n",
    "\n",
    "To achieve that, we divide the data into:\n",
    "- **Train set** (on which we train the model)\n",
    "- **Test set** (on which we test the model performance)\n",
    "\n",
    "Choosing a representative subset for the test set is crucial for a correct estimation of the performance of your model. When the dataset is very small or non-uniform, there are more complex techniques to ensure this similarity between the test set and the whole dataset.\n",
    "\n",
    "For the beginning, let's just hope the dataset is uniform and large enough and train on randomly chosen 80% of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: get the train and test x and y by calling the train_test_split on the whole x and \n",
    "#whole y with the test_size of 0.2 and random_state of 75 (ensures reproducibility)\n",
    "\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BONUS : modify split to reflect stratification (could be by several categories)\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression on Rain in Australia dataset (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a short refresher from your lecture:\n",
    "\n",
    "The aim of logistic regression (as a model) is to predict the class of a given data point.\n",
    "\n",
    "The formula for logistic regression can be summed up as applying a sigmoid function to linear regression\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/lin_log.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "image source: https://medium.com/analytics-vidhya/logistic-regression-in-machine-learning-f3a90c13bb41\n",
    "\n",
    "\n",
    "**Linear regression**:\n",
    " \n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/eq1.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Where y is the target variable and x1, x2 ... and Xn are the other dataset variables.\n",
    "\n",
    "**Sigmoid function**, also called logistic function gives an ‘S’ shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO. \n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/eq2.png\" width=\"100\" height=\"50\">\n",
    "\n",
    "Thus the overall equation is:\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/eq3.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "As the output of the last equation, we get a probability that our data point belongs to the positive class.\n",
    "As mentioned, we can either use the probabilities directly, or set a threshold and get a binary output (whether the data point is predicted to be of class 0 or class 1)\n",
    "\n",
    "Luckily, we don't have to remember these equations, as logistic regression is already implemented in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training the model (5 min)\n",
    "\n",
    "Now let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: create our model, save it into variable lr using the LogosticRegression function\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "\n",
    "\n",
    "# ToDo: train the model by using lr.fit and given the function our x_train and y_train as input:\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model predictions on test set (5 min)\n",
    "\n",
    "After training the model we can use it to predict the classes of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: predict the model results on the test set using the lr.predict function\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "\n",
    "\n",
    "# ToDo: predict the model probabilities per class on the test set using the lr.predict_proba function\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Evaluation of the model (15 mins)\n",
    "\n",
    "After training a model it is crucial to evaluate the model - which means checking how well the final model performs the task for which it was trained. We cannot trust any findings or predictions of the model without knowing how well it performs. Otherwise we might just get our predictions from a crystal ball\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/crystal.jpeg\" width=\"200\" height=\"100\">\n",
    "\n",
    "Image source: John M Lund Photography Inc | Getty Images\n",
    "\n",
    "To evaluate the model we can use several common metrics:\n",
    "\n",
    "### Accuracy (5 min)\n",
    "\n",
    "Accuracy is the most straigntforward metric of the performance of a classifier.<br>\n",
    "It can be calculated by computing the percentage of data points that have been classified correctly <br>\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/acc.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "Image source: https://www.evidentlyai.com/classification-metrics/multi-class-metrics\n",
    "\n",
    "It has its problems:\n",
    "- it averages between classes, which loses some information\n",
    "- the result is misleading if we have a very unbalanced dataset (much more datapoints from one class then from another) \n",
    "\n",
    "An example of an unbalanced dataset is, for example, results of screening for a rare disease. By far most people will not have it, and the model might just learn to predict a negative result.\n",
    "\n",
    "Nevertheless, let's start the evaluation with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: calculate the accuracy using the score() method of the classifier with x_test and y_test as parameters\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "print('Test accuracy of sklearn logistic regression library: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix (5 min)\n",
    "\n",
    "It is a table with all the different possible combinations of predicted and actual values, where each square shows the number of data points which have that combination. It allows us to understand the perfoormance for each class.  <br>\n",
    "\n",
    "For a 2 class classification, the 4 options are true positive, true negative, false positive and false negative.\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/conf_new.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Image source: https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo:  compute the confusion matrix using the confusion_matrix() function using the real and predicted target \n",
    "#values as parameters\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,2))\n",
    "\n",
    "# ToDo: Visualize the confusion matrix cm using the seaborn heatmap \n",
    "#with parameters annot=True, annot_kws={\"size\": 10}, fmt='d', cmap=\"Blues\", ax=ax\n",
    "\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model coefficients evaluation (15 min)\n",
    "\n",
    "When training a model, it is important to check what the model predictions are based on.\n",
    "This could help us in evaluating and validating the model, as well as providing useful information.\n",
    "\n",
    "Let's look at the coefficients of out model that represent the importance of each feature to the result, and compare it to the correlation that we have calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: get the coefficients of the logistic regression using the coef_ method in the logistic regression object\n",
    "#There is a list within a list, so use [0] to take the inside list\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "feature_names = list(x_test)\n",
    "\n",
    "#We sort the coefficients and feature names together\n",
    "cfs, feat_names = zip(*sorted(zip(coeffs, feature_names)))\n",
    "\n",
    "for i in range(len(coeffs)):\n",
    "    print(\"{0}: {1}\".format(feat_names[i], cfs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision/recall (10 min)\n",
    "\n",
    "Precision and recall are characteristics of our model.<br>\n",
    "<u>Precision </u>calculates how many of the data points that are classified as positive are true positives.<br>\n",
    "<u>Recall</u> calculates how many of the positive data points have been classified as positive.<br>\n",
    "They represent different aspects of the model performance.<br>\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/prec_recall.png\" width=\"300\" height=\"150\">\n",
    "\n",
    "Image source: https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: plot the precision-recall curve using the precision_recall_curve function\n",
    "#with the real test target values y_test and the predicted probabilities of the positive class probs_y[:, 1]\n",
    "#Save the values returned by the function into variables precision, recall, thresholds\n",
    "\n",
    "#insert your own code here\n",
    "\n",
    "\n",
    "plt.fill_between(recall, precision)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.title(\"Train Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 score (5 min)\n",
    "\n",
    "The F1 score combines precision and recall using their <u>harmonic mean</u>, and maximizing the F1 score implies simultaneously maximizing both precision and recall. Thus, the F1 score has become the choice of researchers for evaluating their models in conjunction with accuracy. \n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/f1_1.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "Image source: https://encord.com/glossary/f1-score-definition/\n",
    "\n",
    "We can also rewrite it in terms of true positives, true negatives, false positives and false negatives:\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/f1_2.png\" width=\"200\" height=\"100\">\n",
    "\n",
    "Image source: https://towardsdatascience.com/precision-recall-curves-how-to-easily-evaluate-machine-learning-models-in-no-time-435b3dd8939b\n",
    "\n",
    "In sklearn, it is possible ton calculate the f1 score in several ways:\n",
    "- 'binary':\n",
    "Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "- 'micro':\n",
    "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "- 'macro':\n",
    "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "- 'weighted':\n",
    "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "- 'samples':\n",
    "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "\n",
    "Let is use compare them and evaluate our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: calucate and print the f1 score using the imported f1_score function with different average parameters\n",
    "\n",
    "#Insert your own code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the Receiver Operating Characteristic Curve (ROC AUC) (10 min)\n",
    "<br>\n",
    "ROC AUC is a curve that shows how easily we can separate our classes. <br>\n",
    "<br>\n",
    "The curve is computed by calculating the number true positives (correctly classified \"positive\" examples) and false positives (negative examples that have been classified as positive). <br>\n",
    "\n",
    "We have not just one value, but a curve because we calculate those numbers for each possible probability threshold (from 0 to 1) that we use to separate the classes. <br>\n",
    "\n",
    "<u>Example threshold</u>: 0.5 (if the probability is higher that 0.5, we assign a data point to a positive class.<br>\n",
    "                   0 - everything is assigned to the positive class<br>\n",
    "                   1 - everything is assigned to the negative class<br>\n",
    "\n",
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/images/ROC_AUC.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Image source: https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/\n",
    "\n",
    "Now let's plot the ROC curve of our model and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: use the metrics.roc_curve function to calculate the false positive rates into the variable fpr, \n",
    "#true positives rates into the variable tpr and thresholds into variable thresholds\n",
    "#Use the probabilities of class 1 (probs_y[:, 1])\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "\n",
    "#ToDo: calculate the area under the ROC curve using the metrics.roc_auc_score function with parameters y_test \n",
    "#and the probabilistic predictions for the positive class (save it into the variable auc_score)\n",
    "\n",
    "#Insert your own code here\n",
    "\n",
    "\n",
    "#Let's plot the ROC curve and print the ROC AUC score\n",
    "plt.style.use('seaborn') \n",
    "plt.plot(fpr, tpr, linestyle='--',color='orange', label='Logistic Regression')\n",
    "plt.plot([0, 1], [0, 1], 'k-', lw=1,dashes=[2, 2], label='Random Classifier')\n",
    "plt.title('ROC curve (ROC AUC score {0})'.format(auc_score))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive rate') \n",
    "plt.legend(loc='best')\n",
    "plt.savefig('ROC',dpi=300)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BONUS EXCERCISE : improve model (hyperparameters?)\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "for c_temp in Cs: \n",
    "    lr = LogisticRegression(C=c_temp)\n",
    "    lr.fit(x_train, y_train)\n",
    "    y_pred = lr.predict(x_test)\n",
    "    probs_y=lr.predict_proba(x_test)\n",
    "    acc = lr.score(x_test, y_test)\n",
    "    print('Test accuracy of sklearn logistic regression library for C={}: {}'.format(c_temp, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire\n",
    "Hope you enjoyed the exercise! \n",
    "Before you leave, please fill in our questionnaire (link via the qr code below).\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/questionnaires/LogRegr_question_qr.png\" width=\"200\" height=\"100\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6012,
     "sourceId": 1733506,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 23648,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
