{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZSd4slQ4jpd"
   },
   "source": [
    "# Linear Regression (~ 120 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weOiMwGbY_VS"
   },
   "source": [
    "![img](https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/boston.jpeg)\n",
    "\n",
    "**Background:**\n",
    "\n",
    "Predicting housing prices holds significant importance for various stakeholders in the real estate market. Homeowners can leverage this information to make informed decisions about selling, determining listing prices, and deciding on property investments. For potential buyers, understanding housing prices aids in making informed choices about suitable properties, crafting reasonable offers, and assessing affordability. Real estate professionals rely on accurate price predictions to assist clients in buying or selling homes and making strategic investment decisions. Machine learning, with its ability to provide precise and reliable estimates, emerges as a valuable tool for enhancing housing price predictions compared to traditional methods.\n",
    "\n",
    "**Dataset:**\n",
    "The [Boston Housing Dataset] (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) is a publicly available dataset containing 506 records with 14 attributes. These attributes include key characteristics such as per capita crime rate, residential zoning ratio, non-retail business square footage, proximity to the Charles River, nitrogen oxide concentration, average number of rooms per dwelling, age of units, distance to employment centers, access to radial highways, property tax rates, pupil-teacher ratios, racial proportions, low-income population, and, significantly, the median value of owner-occupied homes in $1000's (MEDV).\n",
    "\n",
    "**Features:**\n",
    "\n",
    "1. CRIM - Per capita crime rate by town\n",
    "2. ZN - Proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS - Proportion of non-retail business acres per town.\n",
    "4. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "5. NOX - Nitric oxides concentration (parts per 10 million)\n",
    "6. RM - Average number of rooms per dwelling\n",
    "7. AGE - Proportion of owner-occupied units built before 1940\n",
    "8. DIS - Weighted distances to five Boston employment centers\n",
    "9. RAD - Index of accessibility to radial highways\n",
    "10. TAX - Full-value property-tax rate per 10,000 USD\n",
    "11. PTRATIO - Pupil-teacher ratio by town\n",
    "12. B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT - Percentage lower status of the population\n",
    "14. MEDV - Median value of owner-occupied homes in $1000's\n",
    "\n",
    "**Task:**\n",
    "Your task is to develop a predictive model to estimate the median value of owner-occupied homes in $1000's (MEDV) based on the provided dataset. Leveraging machine learning techniques, your goal is to create an accurate and reliable model that enhances predictions in comparison to traditional approaches. This exercise will enable you to apply regression analysis and explore the relationships between various features and the target variable (MEDV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubZhAG4Bu6TN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data manipulation and analysis\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-dWd1hc7r-K"
   },
   "source": [
    "ToDo (10 min):\n",
    "\n",
    "*   What is each library helpful for? (You can use Google to familiarise yourself with the libraries)\n",
    "*   Which of the lines above imports a package? Which imports a function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHdBi9fz4gxF"
   },
   "source": [
    "##  Data Loading and Initial Exploration (8 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqDreUTwaOLB"
   },
   "source": [
    "**Objective:**\n",
    "This exercise focuses on loading a dataset, gaining familiarity with its structure using `pd.head()`, and performing a simple data cleaning task by removing a redundant index column.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Load the Dataset:**\n",
    "   - Use `pd.read_csv()` to read the dataset we provided.\n",
    "\n",
    "2. **Familiarize Yourself:**\n",
    "   - Utilize the pandas function `head()` to display the first 5 records of the dataset.\n",
    "   - Analyze the columns and their potential meanings.\n",
    "\n",
    "3. **Remove Redundant Index Column:**\n",
    "   - Identify the redundant index column, \"Unnamed: 0.\"\n",
    "   - Use the Pandas `drop()` function to remove this column.\n",
    "\n",
    "**Analysis:**\n",
    "Examine the output to confirm the successful removal of the \"Unnamed: 0\" column and ensure that the dataset is ready for further exploration. Understanding the columns is essential for subsequent analysis and modeling tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of0gt9q7gVuh"
   },
   "outputs": [],
   "source": [
    "# Download the datset from github\n",
    "!curl -O -L https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/1.%20Linear_regression/Boston.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akhmaDcpvUgr"
   },
   "outputs": [],
   "source": [
    "# 1. Read the dataset\n",
    "#...Your code goes here\n",
    "\n",
    "# 2. Print the first 5 records of the dataset\n",
    "#...Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnOWwdOgInWZ"
   },
   "outputs": [],
   "source": [
    "# 3. Clean the dataset\n",
    "col_names = dataset.columns.to_list()\n",
    "print(col_names)\n",
    "dataset = #...Your code goes here\n",
    "col_names = dataset.columns.to_list()\n",
    "print(col_names)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q433L7i8wlc"
   },
   "source": [
    "## Data Exploration (20 min)\n",
    "\n",
    "**Objective:**\n",
    "In this exercise, we aim to perform exploratory data analysis by visualizing the correlations between the features using scatter plots and histograms. Familiarity with these types of plots is crucial for gaining insights into the dataset.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Scatter Plots vs. Histograms:**\n",
    "   - Briefly explain the differences between scatter plots and histograms.\n",
    "   - Utilize the `matplotlib` library to create these plots.\n",
    "\n",
    "2. **Correlation Visualization:**\n",
    "   - Considering that there are 14 features, we need to generate a total of 196 plots to visualize the correlations between each feature.\n",
    "   - Complete the missing parts in the code snippet provided below.\n",
    "   - Use `axs[COLUMN_INDEX, ROW_INDEX]` to select subplots efficiently.\n",
    "\n",
    "**Note:** The correlation plots will aid in identifying relationships between different features, guiding us in the selection of relevant features for further analysis.\n",
    "\n",
    "\n",
    "**Analysis:**\n",
    "Examine the generated plots to identify potential correlations and patterns between different features. This step is crucial for feature selection and gaining a preliminary understanding of the dataset's structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1RZYHR4wAfS"
   },
   "outputs": [],
   "source": [
    "# making a super plot with 14 by 14 subplots\n",
    "fig, axs = plt.subplots(len(col_names), len(col_names), figsize=(25, 25))\n",
    "\n",
    "for i in range(len(col_names)):\n",
    "  for j in range(len(col_names)):\n",
    "    if i == j: # if identical plot histogram\n",
    "      #...Your code goes here\n",
    "    else: # else plot the correlation by using a scatter plot\n",
    "      #...Your code goes here\n",
    "\n",
    "    axs[0,j].set_title(col_names[j])\n",
    "  axs[i,0].set(ylabel=col_names[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQlyioSA9ZcC"
   },
   "source": [
    "### Your observations:\n",
    "*Make some notes on what you can observe in the plot. E.g. positive-, negative correlation, value ranges, outliers. Have a look at the variables explanation above as well. Can you explain some of your observations?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrQ5hVPs-TGd"
   },
   "source": [
    "## Linear Regression Exploration (Analytical solution) (6 min)\n",
    "\n",
    "**Objective:**\n",
    "After becoming familiar with the data set, the goal is to delve into the basics of linear regression through an analytical solution. For simplicity, we will focus on using a single independent variable to predict a target variable, allowing us to visualize the relationships in 2D space. It's important to note that linear regression assumes a linear relationship between input and output.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Select variables:**\n",
    "   - Reflect on your observations of the data set.\n",
    "   - Select two variables that you believe have a linear relationship. These variables will be used as the independent and target variables in our linear regression exploration.\n",
    "\n",
    "2. **Variable Selection:**.\n",
    "   - Enter the names of the selected independent and target variables in the cell provided below.\n",
    "   - This selection will guide our subsequent analysis and allow us to more closely examine the potential linear relationship.\n",
    "\n",
    "**Note:** Linear regression assumes that the relationship between variables is linear. Therefore, the variables selected should be those for which a linear relationship is expected.\n",
    "\n",
    "```python\n",
    "# Fill in the chosen variable names\n",
    "independent_variable (predictor) = \"Your_Independent_Variable\"\n",
    "target_variable (target) = \"Your_Target_Variable\"\n",
    "```\n",
    "\n",
    "**Analysis:**\n",
    "Continue with a visual exploration and analytical solution of the linear regression parameters for the selected variables. This exercise is designed to increase your understanding of the basic principles of linear regression and lay the foundation for more complex analyses in future modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux943LN35N1g"
   },
   "outputs": [],
   "source": [
    "predictor = #TODO: fill in a column name\n",
    "target = #TODO: fill in a column name\n",
    "\n",
    "X = np.array(dataset[predictor].tolist()).reshape(-1, 1)\n",
    "y = np.array(dataset[target].tolist()).reshape(-1, 1)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"X, the input, Predictor\")\n",
    "plt.ylabel(\"y, the output, Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5apNOTKm_W-v"
   },
   "source": [
    "**Implementing the Analytical Solution for Linear Regression**: In this task, you are required to implement the analytical solution for fitting a line through a dataset. The problem can be expressed as a system of equations:\n",
    "\n",
    "$$y_1 = b + w \\cdot x_1$$\n",
    "$$\\vdots$$\n",
    "$$y_n = b + w \\cdot x_n$$\n",
    "\n",
    "Your goal is to find the optimal values for the weight $w$ and bias $b$ that best fit the given dataset. A line is defined by the equation $y = b + wx$, where $w$ is the weight or slope of the line, and $b$ is the bias term, representing the point where the line intersects the $y$-axis (when $x = 0$). A positive value for $m$ indicates an upward slope as $x$ increases, while a negative value results in a downward slope.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. **Fill in Weight and Bias:**\n",
    "   - In the code cell below, you'll find initial values for the weight $w=w_{\\text{initial}}$ and bias $b=b_{\\text{initial}}$.\n",
    "   - Your task is to modify these values to find the optimal parameters that best fit the dataset.\n",
    "\n",
    "2. **Visualization:**\n",
    "   - The code includes a scatter plot of the dataset and a line based on the initial weight and bias.\n",
    "   - Adjust the weight $w=w_{\\text{initial}}$ and bias $b=b_{\\text{initial}}$ to observe how the line changes in an attempt to fit the dataset better.\n",
    "\n",
    "3. **Experimentation:**\n",
    "   - Play around with different values for $w=w_{\\text{rand}}$ and $b=b_{\\text{rand}}$.\n",
    "   - Use the formulas for the analytical solutions to find the weight $w=w_{\\text{analy}}$ and $b=b_{\\text{analy}}$ and implement them\n",
    "   - Observe how the line changes direction and position relative to the dataset.\n",
    "   - Understand the impact of positive and negative values for the weight.\n",
    "\n",
    "\n",
    "**Analysis:**\n",
    "Experiment with different values for weight and bias to observe how the line adjusts to the dataset. This task simulates the manual exploration of parameters, which is a fundamental step in understanding the principles of linear regression. Remember that in practical scenarios, machine learning algorithms are employed to automate the process of finding optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGr060dtP_0"
   },
   "outputs": [],
   "source": [
    "# Attempt to estimate the best-fitting line\n",
    "weight = #TODO: fill in a number\n",
    "bias = #TODO: fill in a number\n",
    "\n",
    "lineStart_hand = X.min() * weight + bias\n",
    "lineEnd_hand = X.max() * weight + bias\n",
    "plt.scatter(X, y)\n",
    "plt.plot([X.min(), X.max()], [lineStart_hand, lineEnd_hand], color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDp7RvluxkA1"
   },
   "source": [
    "### Evaluation Metrics Overview (10 min)\n",
    "\n",
    "To assess the quality of our linear regression model and how effectively it fits the data, we employ key evaluation metrics, notably $R^2$ (coefficient of determination) and Mean Squared Error (MSE).\n",
    "\n",
    "**Coefficient of Determination ($R^2$):**\n",
    "$R^2$ is calculated as $(1 - \\frac{u}{v})$, where:\n",
    "- $u$ is the residual sum of squares, given by ```((y_true - y_pred)** 2).sum()```.\n",
    "- $v$ is the total sum of squares, calculated as ```((y_true - y_true.mean()) ** 2).sum()```.\n",
    "\n",
    "A perfect model would yield an $R^2$ score of 1.0, while a score of 0.0 indicates that the model is equivalent to a constant predictor, and negative values are possible if the model performs worse than a simple constant prediction.\n",
    "\n",
    "*Source: [scikit-learn Linear Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)*\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "MSE is a widely used metric, representing the expected value of the squared error. It is defined as $\\frac{1}{N} ∑_{i=0}^{N-1}(y_i - \\hat{y}_i)^2$, where $\\hat{y}$ represents the model predictions. A lower MSE indicates better performance, and achieving an MSE of 0 is the optimal outcome, signifying that predictions perfectly match the target values.\n",
    "\n",
    "These evaluation metrics offer valuable insights into the accuracy and effectiveness of our linear regression model. While $R^2$ provides a measure of the proportion of the variance in the target variable that is predictable from the independent variables, MSE quantifies the average squared difference between predicted and actual values.\n",
    "\n",
    "*Note: The choice of metrics depends on the specific goals and requirements of the modeling task.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oD8YYcgLuhO9"
   },
   "outputs": [],
   "source": [
    "def reg_score(X, y, weight, bias):\n",
    "  u_f = lambda X, y: (y - (X * weight + bias))**2\n",
    "  u = u_f(X, y).sum(axis=0)\n",
    "  v = ((y - y.mean()) ** 2).sum(axis=0)\n",
    "  r = 1 - (u/v)\n",
    "  return r\n",
    "\n",
    "def mse(X, y, weight, bias):\n",
    "    mse = np.power((X * weight + bias)-y, 2)\n",
    "    mse = mse.sum(axis=0)\n",
    "    return mse/X.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4dVJJhT8fTn"
   },
   "source": [
    "Let's see how your model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JWZ7I54G9fn"
   },
   "outputs": [],
   "source": [
    "print(\"R^2: \", reg_score(X, y, weight, bias).item())\n",
    "\n",
    "print(\"MSE: \", mse(X, y, weight, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk597n-Q9-DI"
   },
   "source": [
    "**Exploring the Impact of Weight and Bias on Model Performance:**\n",
    "\n",
    "Let's take a closer look at why the numbers matter in our modeling adventure. Our mission is to fine-tune our model's performance, and two key players in this game are the *weight* and *bias*. Think of them as our navigators through a landscape represented by the cost function.\n",
    "\n",
    "The cost function/metric is like a bumpy terrain, and the *weight* and *bias* are our guides to finding the smoothest path, where our model performs at its best. In the upcoming plots, you'll see a visual representation of this journey. We're essentially testing different combinations of *weight* and *bias* to pinpoint the sweet spot, where our model is optimized for success. It's like finding the perfect balance to make our model predict outcomes as accurately as possible. So, let's dive into the visuals and witness how these simple adjustments can lead us to a better, more efficient model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUdOvOkIZ5RU"
   },
   "outputs": [],
   "source": [
    "def plot_contour(w_min, w_max, b_min, b_max, metric_function, title, resolution=100, cmap='coolwarm'):\n",
    "  ws = np.linspace(w_min, w_max, resolution) #slope range\n",
    "  bs = np.linspace(b_min, b_max, resolution) #intercept\n",
    "  WS, BS = np.meshgrid(ws, bs)\n",
    "\n",
    "  Z = metric_function(X, y, WS.flatten(), BS.flatten())\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  cs = ax.contourf(WS, BS, Z.reshape(WS.shape), cmap=cmap)\n",
    "  ax.grid(c='k', ls='-', alpha=0.3)\n",
    "  ax.set_title(title)\n",
    "  ax.set_xlabel(\"Weight\")\n",
    "  ax.set_ylabel(\"Bias\")\n",
    "  plt.colorbar(cs)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JyPN5yioR1y"
   },
   "outputs": [],
   "source": [
    "plot_contour(-10, 10, -50, 50, reg_score, 'Contour plot of R-squared')\n",
    "plot_contour(-10, 10, -50, 50, mse, 'Contour plot of MSE', cmap='coolwarm_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3H7oQqV0Ut2"
   },
   "source": [
    "**Analysis:**\n",
    "Examining the contour plots, it's evident that we're navigating the landscape of our cost function, seeking the optimal combination of weight and bias for our model. The plots provide a visual representation of our journey, showcasing the varying performance scores corresponding to different weight-bias configurations.\n",
    "\n",
    "As you experiment with different weight and bias values, take note of how far you are from the optimum – the point where your model achieves the best possible performance. Iteratively try different combinations to see if you can enhance the fit and move closer to the optimal solution. Keep track of your attempts and their resulting scores in the cell below. This record will serve as a valuable log of your exploration, helping you understand the impact of different parameter choices on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSLRMNAXBL0m"
   },
   "source": [
    "| Weight | Bias |  R^2 | MSE |\n",
    "|--------|------|------|-----|\n",
    "|        |      |      |     |\n",
    "|        |      |      |     |\n",
    "|        |      |      |     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-edjm7TW8q3v"
   },
   "source": [
    "## Linear Regression (Iterative solution) (2 min)\n",
    "\n",
    "If you reach a point where further improvement seems challenging, consider leveraging the `LinearRegression` function from the `scikit-learn` library. This ready-made tool can automate the process, potentially providing a more efficient and optimized solution. For more details, refer to [scikit-learn Linear Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Let's transition to this phase and explore the benefits it offers in streamlining our linear regression modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuJPN8-zBaw4"
   },
   "outputs": [],
   "source": [
    "reg = #TODO use linear regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKcJ_9YgDMlh"
   },
   "source": [
    "Let's see how the model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p_oeD_NDkL9"
   },
   "outputs": [],
   "source": [
    "print(f\"Weight: {reg.coef_.item()} ; Bias:{reg.intercept_.item()}\")\n",
    "print(\"with R^2 score: \", reg.score(X, y), \" and MSE: \", mse(X, y, reg.coef_.item(), reg.intercept_.item()).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB0sxXoe1PK2"
   },
   "source": [
    "##Analytical vs. Iterative Solutions Comparison (14 min):\n",
    "\n",
    "Let's gauge the effectiveness of both the analytical and iterative approaches. How does the score from your analytical solution stack up against using an iterative method? Take a moment to reflect on this numerical comparison.\n",
    "\n",
    "Now, shift your focus to the visual representation below, showcasing the lines derived from the analytical and iterative solutions. Can you easily discern which line performs better in capturing the underlying patterns of the dataset? Consider the nuances in the plot and evaluate if one solution stands out as evidently superior to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q75kUGanD9jX"
   },
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "By1rKQLAEWsw"
   },
   "outputs": [],
   "source": [
    "lineStart = X.min() * reg.coef_[0] + reg.intercept_\n",
    "lineEnd = X.max() * reg.coef_[0] + reg.intercept_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "ax.plot([X.min(), X.max()], [lineStart_hand, lineEnd_hand], color = 'r', label='Analytical')\n",
    "ax.plot([X.min(), X.max()], [lineStart, lineEnd], color = 'g', label='Sklearn')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVqp6yQZ3o7U"
   },
   "source": [
    "**Residual Analysis**\n",
    "\n",
    "Now, let's take a closer look at the fitted line using a technique called residuals analysis. This method enables us to dissect the portion of the data that remains unexplained by our model. The residual for each data point represents the distance from that point to the regression line. Scrutinizing these residuals provides valuable insights into how well our model captures the nuances of the dataset and where it might fall short.\n",
    "\n",
    "In an ideal scenario, where our model perfectly fits the data, all residuals would be zero. However, this rarely occurs. Instead, we anticipate the residuals to be randomly scattered around zero. If there's any discernible pattern in the residual plot, it signals that there's additional information in the data that our model isn't capturing.\n",
    "\n",
    "Moreover, the residual plot serves as a tool to identify outliers in our data. Outliers, being far away from the center line, can be easily spotted through this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9f32WwsUNkI"
   },
   "outputs": [],
   "source": [
    "pred = reg.predict(X)\n",
    "#TODO: scatter plot of the predictions and residuals\n",
    "#...Your code goes here\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.hlines(y=0, xmin=-10, xmax=40, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDZIWpsGlIkp"
   },
   "source": [
    "### Your observations:\n",
    "*Make some notes on what you can observe in the plot.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_UVGPo84Ikq"
   },
   "source": [
    "#Polynomial Regression Exploration (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMKBK65XnCdd"
   },
   "source": [
    "**Objective**: In many real-world scenarios, assuming a linear dependency may not adequately capture the underlying complexities of the data. To illustrate this, examine the plot below and consider how you would describe the shape of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvo3zfkXnssV"
   },
   "source": [
    "*your answer goes here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuJMLTOM2cPQ"
   },
   "outputs": [],
   "source": [
    "X = np.array(dataset['dis'].tolist()).reshape(-1, 1)\n",
    "y = np.array(dataset['nox'].tolist()).reshape(-1, 1)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfgTvD47nyaf"
   },
   "source": [
    "1. **Linear Regression:**\n",
    "   - Begin by fitting a linear regression model to the data as we did previously.\n",
    "   - Evaluate the resulting $R^2$ and Mean Squared Error (MSE) scores to gauge the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKAzMEG_wXmc"
   },
   "outputs": [],
   "source": [
    "# to work with the multi dimmensional data we have to redefine the mse function\n",
    "def mse(X, y, weight, bias):\n",
    "    y = y.reshape(-1)\n",
    "    mse_val = np.sum(X * weight, axis=1)\n",
    "    mse_val = mse_val + bias\n",
    "    mse_val = np.power(mse_val-y, 2)\n",
    "    mse_val = mse_val.sum(axis=0)\n",
    "\n",
    "    return mse_val/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5UbfZFfREpr"
   },
   "outputs": [],
   "source": [
    "#...Your code goes here\n",
    "# Fit the model\n",
    "# print the weight, bias, r^2 and MSE score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_jkP6LsoIH_"
   },
   "source": [
    "2. **Polynomial Regression:**\n",
    "   - Recognizing the limitations of linear assumptions, let's explore polynomial regression. Instead of adhering to a linear equation $y = X \\cdot w + b$, we now consider a polynomial model $y = X \\cdot w_1 + X^2 \\cdot w_2 + \\ldots + X^n \\cdot w_n + b$.\n",
    "   - Utilize the `PolynomialFeatures` function from Scikit-Learn to facilitate this transformation (check the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)).\n",
    "\n",
    "3. **Degree 2 Polynomial:**\n",
    "   - Begin by testing a polynomial of degree 2: $y = X \\cdot w_1 + X^2 \\cdot w_2 + b$.\n",
    "   - Observe how the weight and bias terms differ compared to the linear model.\n",
    "   - Evaluate the $R^2$ and MSE scores for the polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLiEK977RMly"
   },
   "outputs": [],
   "source": [
    "degree = 2\n",
    "poly = PolynomialFeatures(degree)\n",
    "T = poly.fit_transform(X)\n",
    "T = T[:,1:] # remove the first column as this is always 1\n",
    "\n",
    "reg = LinearRegression().fit( T, y)\n",
    "print(f\"Weights: {reg.coef_[0]} ; Bias:{reg.intercept_.item()}\")\n",
    "print(\"with R^2 score: \", reg.score(T, y), \" and MSE: \", mse(T, y, np.tile(reg.coef_[0], (len(T), 1)), reg.intercept_.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "td3Qf1KPpqRK"
   },
   "source": [
    "**Analysis**: Reflect on the changes in the weight and bias terms, and how the scores compare between the linear and polynomial models. This exploration provides valuable insights into the adaptability of polynomial regression in capturing more intricate relationships within the data.\n",
    "\n",
    "Try a few more degrees by editing the cell and see how the sores change. To keep track of your changes fill in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HLvPHrcp71p"
   },
   "source": [
    "*your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGhkxW_bqZm8"
   },
   "source": [
    "|Degree | Weight Vector | Bias |  R^2 | MSE |\n",
    "|-------|---------------|------|------|-----|\n",
    "|       |               |      |      |     |\n",
    "|       |               |      |      |     |\n",
    "|       |               |      |      |     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WasZCNdb2Tp_"
   },
   "source": [
    "# Exploring Multivariate Regression (12 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8bhQsSAHJgO"
   },
   "source": [
    "As we've just seen, using just one piece of information doesn't always give us accurate predictions. Life is full of different factors that can affect outcomes. For simplicity's sake, we'll focus on just two factors for now. This way we can visualize it in 3D space, where we can see how two things work together.\n",
    "\n",
    "Take a look at the plot below (you can zoom and pan). This time you need to choose two weights to understand how these factors affect our predictions. As before, try different combinations and see how the plot changes. While we're looking at two factors here, remember that in real life we may be dealing with more factors, and the same techniques we're learning now can be applied to solve more complex problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-N3wkUbRdS-"
   },
   "outputs": [],
   "source": [
    "input_columns = ['dis', 'lstat']\n",
    "X = np.array(dataset[input_columns].values)\n",
    "y = np.array(dataset['medv'].tolist()).reshape(-1, 1)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(x=dataset[input_columns[0]], y=dataset[input_columns[1]], z=dataset['medv'], mode='markers', marker=dict(\n",
    "        size=4,\n",
    "        color='blue',\n",
    "        opacity=0.8\n",
    "    )))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q6N8Bo2nXol"
   },
   "source": [
    "- Play with the weights and bias (now represented as a matrix) to visualize the fit space. Notice how the scatter points align with this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gamW-ZxY_pdP"
   },
   "outputs": [],
   "source": [
    "weights = #TODO: your weights\n",
    "bias = #TODO: your bias\n",
    "\n",
    "# create a meshgrid to plot the decision plane\n",
    "xs = np.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "ys = np.linspace(X[:,1].min(), X[:,1].max(), 100)\n",
    "xs, ys = np.meshgrid(xs, ys)\n",
    "\n",
    "zs = xs * weights[0] + ys * weights[1] + bias\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_surface(x=xs,y=ys,z=zs,showscale=False,showlegend=False, colorscale ='Reds')\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=dataset[input_columns[0]], y=dataset[input_columns[1]], z=dataset['medv'], mode='markers', marker=dict(\n",
    "        size=4,\n",
    "        color='blue',\n",
    "        opacity=0.8\n",
    "    )))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K96xRdY8SELD"
   },
   "outputs": [],
   "source": [
    "print(\"with R^2 score: \", reg_score(X, y, weights, bias), \" and MSE: \", mse(X, y, weights, bias)) #delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqOBa1oxncvZ"
   },
   "source": [
    "- Next, use the Multivariate Regression ``LinearRegression`` function to see how well the fitted space matches the actual data. As before, use metrics such as $R^2$ and Mean Squared Error to evaluate the goodness of fit. Experiment and observe how adjusting the weights affects the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GwNuu0pAJ2Q"
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit( X, y)\n",
    "\n",
    "# The coefficients\n",
    "print(f\"Weights: {reg.coef_[0]} ; Bias:{reg.intercept_}\")\n",
    "print(\"with R^2 score: \", reg.score(X, y), \" and MSE: \", mse(X, y, reg.coef_, reg.intercept_.item()))\n",
    "\n",
    "zs_model = xs * reg.coef_[0][0] + ys * reg.coef_[0][1] + reg.intercept_.item()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_surface(x=xs,y=ys,z=zs,showscale=False,showlegend=True, colorscale ='Reds', name='Analytical')\n",
    "fig.add_surface(x=xs,y=ys,z=zs_model,showscale=False,showlegend=True, colorscale ='Greens', name= 'Model')\n",
    "\n",
    "fig.add_trace(go.Scatter3d(x=dataset[input_columns[0]], y=dataset[input_columns[1]], z=dataset['medv'], mode='markers', marker=dict(\n",
    "        size=4,\n",
    "        color='blue',\n",
    "        opacity=0.8\n",
    "    ), name = 'Data'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDNZbbhObsQ8"
   },
   "source": [
    "Compare the two planes and the scores. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM9lWq6Ab1Pw"
   },
   "source": [
    "*Your answer goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yf0CDrMq4YGT"
   },
   "source": [
    "# Your Regression Model (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svk4oAoiXunM"
   },
   "source": [
    "Having gained valuable insights into the available data, it's time to focus on our **ultimate goal**: predicting the median value of owner-occupied homes. Use the correlation plot below to identify the most influential features for training a regression model. Feel free to explore additional techniques such as polynomial modeling or feature engineering to improve model performance. Embrace creativity and strive for optimal results as you move forward in your quest for accurate predictions.\n",
    "After training your own model, apply the trained model to new data entries to predict housing prices and evaluate its real-world predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA0tNIwVW5q0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "correlation_matrix = dataset.corr().round(2)\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "sns.heatmap(data=correlation_matrix, annot=True, ax=ax)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAlTG4YwyV-V"
   },
   "outputs": [],
   "source": [
    "#TODO: train a regression model based on your selected features\n",
    "#...Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4oTUXYaYaAp"
   },
   "outputs": [],
   "source": [
    "print(\"with R^2 score: \", reg_score(X, y, weights, bias), \" and MSE: \", mse(X, y, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uuA4s1hH4pre"
   },
   "outputs": [],
   "source": [
    "# Predict your housing prices\n",
    "input = np.array([\"enter your datapoints here\"]).reshape(1,-1)\n",
    "\n",
    "#TODO: make the prediction and print it\n",
    "# Your code goes here...\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
