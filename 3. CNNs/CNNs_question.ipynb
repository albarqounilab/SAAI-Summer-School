{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b996d574",
      "metadata": {
        "id": "b996d574"
      },
      "source": [
        "# CNNs (Convolutional Neural Networks)\n",
        "\n",
        "In this notebook, we will introduce the concept, define and then get familiar with neural networks.\n",
        "Neural networks are a subfield of machine learning and are what we commonly think of when mentioning deep learning algorithms. Their name \"neural networks\" refers to the fact that the idea behind the algorith was inspired by the human brain, specifically by the way biological neurons exchange information.\n",
        "\n",
        "<img src=\"https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/nn.jpg\" width=\"400\">\n",
        "\n",
        "We will start with multilayer perceptrons and work up to convolutional neural networks, one of the most popular types of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef0dae6",
      "metadata": {
        "id": "4ef0dae6"
      },
      "source": [
        "# Multilayer Perceptrons (30-40 mins)\n",
        "\n",
        "## Introduction (1 min)\n",
        "\n",
        "The traditional definition for an MLP is \"A multilayer perceptron (MLP) is a stack of perceptrons, each of which involves the non-differentiable Heaviside function. This makes such models difficult to train, which is why they were never widely used. MLP is also defined as a fully connected class of feedforward neural network.\"\n",
        "\n",
        "To get comfortable with the concept, check out the image from [TensorFlow's interactive demo - \"Tinker With a Neural Network Right Here in Your Browser.\"](https://playground.tensorflow.org/)\n",
        "\n",
        "![img](https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/vsxnkaR.png)\n",
        "\n",
        "We will start on the MNIST dataset, which you are already familiar with from the previous notebook.\n",
        "However, for the purpose of training a neural network it is often convenient to put the dataset into a DataLoader format, which delivers data to the model a few data points at a time (they are called batches)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c89018",
      "metadata": {
        "id": "03c89018"
      },
      "source": [
        "## Multi-Layer Perceptron for MNIST Classification¶\n",
        "\n",
        "Let's import the necessary libraries for working with data and PyTorch.\n",
        "Torch is the library related to the neural network.\n",
        "You will recognise numpy from the previous notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b07e7a",
      "metadata": {
        "id": "98b07e7a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "#Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432c1f35",
      "metadata": {
        "id": "432c1f35"
      },
      "source": [
        "### Load and Visualize the Data (5 mins)\n",
        "\n",
        "The first step is to download and load the MNIST dataset, just like in the previous notebook.\n",
        "We will create separate DataLoaders for the training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be1fe6d",
      "metadata": {
        "id": "3be1fe6d"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "# How many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# Convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39502ed0",
      "metadata": {
        "id": "39502ed0"
      },
      "source": [
        "To see how the DataLoader format works and check that we have done it correctly, let's visualize a batch of training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1447c5e",
      "metadata": {
        "id": "a1447c5e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# Plot the images in the batch, along with the true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "    #ToDo: plot the image images[idx]\n",
        "    #Insert your own code here\n",
        "\n",
        "    #ToDo: set the title of this axis to the correct image label\n",
        "    #You should use labels[idx].item() to get the label, and dont't forget to convert the value to string.\n",
        "    #INsert your own code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b17cebc",
      "metadata": {
        "id": "3b17cebc"
      },
      "outputs": [],
      "source": [
        "The key idea is to either flatten the image into a\n",
        "fixed-dimensional vector for the rest of th training process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae9dc71f",
      "metadata": {
        "id": "ae9dc71f"
      },
      "source": [
        "### Define the Network Architecture¶ (5 mins)\n",
        "\n",
        "We will define our MLP architecture. For this example, we will use two hidden layers and ReLU (rectified linear unit) activation to avoid overfitting.\n",
        "\n",
        "Overfitting, for reference, is a type of error in machine learning or statistics, which happens when a model is too perfectly fitted to a limited set of data points. The negative consequence is that the model is only useful on the training data set, and does not generalise on any other data sets.\n",
        "\n",
        "For more information on overfitting, see https://www.investopedia.com/terms/o/overfitting.asp.\n",
        "\n",
        "A rectified linear unit (ReLU) is an activation function that introduces the property of nonlinearity to a deep learning model and solves the vanishing gradients issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d85fb3",
      "metadata": {
        "id": "d3d85fb3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Linear layers\n",
        "        self.fc1 = nn.Linear(28 * 28, 64)\n",
        "        self.fc2 = nn.Linear(64, 16)\n",
        "        self.fc3 = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # Add hidden layers with ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # Output layer with logits (no activation)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# ToDo: use the Net function to initialize the MLP into the variable model\n",
        "#Insert your own code here\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab6c9f8",
      "metadata": {
        "id": "cab6c9f8"
      },
      "source": [
        "### Specify Loss Function and Optimizer (1 min)\n",
        "\n",
        "Key elements of defining and training a model is choosing a loss function (the value that the model will try to minimise during the training process), optimiser (the algorithm which defines how the model is updated) and connected learning rate. We need to be careful in how we choose the learning rate in order to achieve\n",
        "convergence. Choosing the learning rate is a critical aspect of training machine learning models, especially in deep learning. The learning rate determines the step size at which the model updates its parameters during training. If the learning rate is too small, the model may converge very slowly, while if it's too large, the model might not converge at all or even diverge.\n",
        "\n",
        "<img src=\"https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/Eyh0mH4.png\" width=\"400\">\n",
        "\n",
        "Source: https://cs231n.github.io\n",
        "        \n",
        "For classification tasks, cross-entropy loss is commonly used, along with an optimizer like stochastic gradient descent (SGD), which is the basic option,  or Adam, which is most often used. For more details on optimisers, see https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/\n",
        "\n",
        "![image](https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/opt.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99183ea4",
      "metadata": {
        "id": "99183ea4"
      },
      "outputs": [],
      "source": [
        "# Specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "#ToDo: try to change the learning rate to 0.01\n",
        "#Insert your own code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa22a146",
      "metadata": {
        "id": "fa22a146"
      },
      "source": [
        "### Train the Network (5 mins)\n",
        "\n",
        "Now, we will train our model for a certain number of epochs (which is defined by the model having trained on every data point exactly once), monitoring the training loss. The model will learn to classify the hand-written digits from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b9ba2c",
      "metadata": {
        "id": "f2b9ba2c"
      },
      "outputs": [],
      "source": [
        "# Number of epochs to train the model\n",
        "n_epochs = 5\n",
        "\n",
        "model.train()  # Prep model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # Monitor training loss\n",
        "    train_loss = 0.0\n",
        "\n",
        "    # Train the model\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()  # Clear the gradients of all optimized variables\n",
        "\n",
        "        #ToDo: perform forward pass, which means running the data through the model\n",
        "        #Insert your own code here\n",
        "\n",
        "        #ToDo: calculate the loss function (via the defined function criterion, using output and target values)\n",
        "        #Insert your own code here\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Perform a single optimization step\n",
        "        train_loss += loss.item() * data.size(0)  # Update running training loss\n",
        "\n",
        "    # Print training statistics\n",
        "    # Calculate average loss over an epoch\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "\n",
        "    print('Epoch: {}\\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3c5500",
      "metadata": {
        "id": "cf3c5500"
      },
      "source": [
        "### Test the Trained Network (1 min)\n",
        "\n",
        "Finally, we will test our trained model on the test dataset and evaluate its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "633adc90",
      "metadata": {
        "id": "633adc90"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()  # Prep model for evaluation\n",
        "\n",
        "for data, target in test_loader:\n",
        "    output = model(data)  # Forward pass\n",
        "    loss = criterion(output, target)  # Calculate the loss\n",
        "    test_loss += loss.item() * data.size(0)  # Update test loss\n",
        "    _, pred = torch.max(output, 1)  # Convert output probabilities to predicted class\n",
        "\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))  # Compare predictions to true label\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# Calculate and print average test loss\n",
        "test_loss = test_loss / len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of {}: {:.2f}% ({}/{})'.format(\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            class_correct[i], class_total[i]))\n",
        "    else:\n",
        "        print('Test Accuracy of {}: N/A (no training examples)'.format(str(i)))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): {:.2f}% ({}/{})'.format(\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a91a7e1",
      "metadata": {
        "id": "1a91a7e1"
      },
      "source": [
        "### Visualize Sample Test Results (1 min)\n",
        "\n",
        "Let us visualize some sample test results to see how well the model performs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6cc20c",
      "metadata": {
        "id": "ec6cc20c"
      },
      "outputs": [],
      "source": [
        "# Obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Get sample outputs\n",
        "output = model(images)\n",
        "_, preds = torch.max(output, 1)\n",
        "images = images.numpy()\n",
        "\n",
        "# Plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "    #ToDo: print image number idx via the axis\n",
        "    #Insert your own code here\n",
        "\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx] == labels[idx] else \"red\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e997ae",
      "metadata": {
        "id": "a3e997ae"
      },
      "source": [
        "### Bonus Exercise 1: Improving the Model (10 mins)\n",
        "\n",
        "Now that you have trained the model, there are several ways you can further improve it and fine-tune its performance. Here are some exercises you can try:\n",
        "\n",
        "\n",
        "1.   Implement a Validation Scheme: Currently, the model is trained only on the\n",
        "training dataset. You can implement a validation scheme by splitting the training dataset into training and validation sets. This will allow you to monitor the model's performance on unseen data during training and help you avoid overfitting.\n",
        "2.   Hyperparameter Tuning: Experiment with different hyperparameters, such as learning rate, number of hidden layers, number of neurons in each layer, batch size, and number of training epochs. Tuning these hyperparameters can significantly impact the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91f2389",
      "metadata": {
        "id": "d91f2389"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# Number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# How many samples per batch to load\n",
        "batch_size = 20\n",
        "# Percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "# Convert data to torch.FloatTensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# Define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# Prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           sampler=train_sampler, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                           sampler=valid_sampler, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                          num_workers=num_workers)\n",
        "\n",
        "\n",
        "# Define the NN architecture\n",
        "class ImprovedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedNet, self).__init__()\n",
        "        # Number of hidden nodes in each layer\n",
        "        hidden_1 = 1024\n",
        "        hidden_2 = 1024\n",
        "        # Linear layer (784 -> hidden_1)\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
        "        # Linear layer (hidden_1 -> hidden_2)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        # Linear layer (hidden_2 -> 10)\n",
        "        self.fc3 = nn.Linear(hidden_2, 10)\n",
        "        # Dropout layer (p=0.2)\n",
        "        # Dropout prevents overfitting of data\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # Add hidden layer with Leaky ReLU activation function\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        # Add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # Add hidden layer with Leaky ReLU activation function\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        # Add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # Add output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the NN\n",
        "improved_model = ImprovedNet()\n",
        "print(improved_model)\n",
        "\n",
        "# Specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Specify optimizer (Adam) and learning rate = 0.001\n",
        "optimizer = optim.Adam(improved_model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of epochs to train the model\n",
        "n_epochs = 5\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "# Implement validation scheme\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    # Train the model\n",
        "    improved_model.train()\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = improved_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validate the model\n",
        "    improved_model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        output = improved_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()\n",
        "\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    valid_loss = valid_loss / len(valid_loader)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch + 1, train_loss, valid_loss))\n",
        "\n",
        "    # Save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min, valid_loss))\n",
        "        torch.save(improved_model.state_dict(), 'improved_model.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "# Load the model with the lowest validation loss\n",
        "improved_model.load_state_dict(torch.load('improved_model.pt'))\n",
        "\n",
        "# Test the trained network\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "improved_model.eval()\n",
        "for data, target in test_loader:\n",
        "    output = improved_model(data)\n",
        "    loss = criterion(output, target)\n",
        "    test_loss += loss.item() * data.size(0)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "test_loss = test_loss / len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "# Visualize sample test results\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "output = improved_model(images)\n",
        "_, preds = torch.max(output, 1)\n",
        "images = images.numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx] == labels[idx] else \"red\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8eb9d7e",
      "metadata": {
        "id": "a8eb9d7e"
      },
      "source": [
        "## Bonus Excercise 2: CNNs Applications (40 - 60 mins)\n",
        "\n",
        "Convolutional Neural Networks (CNNs) have revolutionized the world of image processing and computer vision. you can explore interactive playgrounds and applications that showcase their remarkable capabilities. let's dive in a couple of examples to understand what CNNs can do:\n",
        "Playgrounds:\n",
        "Image Convolution Playground Experiment with complex image processing operations in your browser.\n",
        "CNN vision Draw numbers and inspect layers.\n",
        "Image kernels Image Kernels Explained Visually\n",
        "Image Convolution Playground Experiment with complex image processing operations in your browser.\n",
        "Play with CNNs:\n",
        "Quick draw Play pictionary with a CNN!\n",
        "AutoDraw CNN helps you draw!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1881c6",
      "metadata": {
        "id": "ca1881c6"
      },
      "source": [
        "# Convolutional Neural Network (CNN) for CIFAR10 Classification (65-85 min)\n",
        "\n",
        "In this notebook, we will build a deep learning Convolutional Neural Network (CNN) model to classify images of the CIFAR10 dataset, a commonly used dataset in computer vision. Your task is to complete each code cell by filling in the appropriate portion of the code. Each cell contains a partial code snippet, and your goal is to provide the missing code to set up the pipeline.\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "![image](https://github.com/albarqounilab/EEDA-Autumn-School/raw/main/images/cifar.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0O3FicsLVlHh",
      "metadata": {
        "id": "0O3FicsLVlHh"
      },
      "source": [
        "### Import packages for the notebook (1 min)\n",
        "\n",
        "In this step, we import the necessary packages for our task. The essential packages used in this notebook are:\n",
        "\n",
        "- numpy as np: NumPy is a fundamental package for scientific computing in Python. We use it for numerical operations and array manipulations.\n",
        "- torch: This is the PyTorch library, which is a popular deep learning framework for building and training neural networks.\n",
        "- torch.nn: This module contains all the necessary functions and classes for building neural networks in PyTorch.\n",
        "- torch.optim: This module contains various optimization algorithms used for training neural networks.\n",
        "- torch.utils.data.DataLoader: This class is used to load data efficiently for training and validation.\n",
        "- torchvision.transforms: This module provides common image transformations like resizing, cropping, and normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4wzfZt7e01E_",
      "metadata": {
        "id": "4wzfZt7e01E_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C-TwuP1x27Kx",
      "metadata": {
        "id": "C-TwuP1x27Kx"
      },
      "source": [
        "### Define a transformation\n",
        "\n",
        "You may add your prefered choices from the `transforms` library or leave it as it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GmGfdMZb28ft",
      "metadata": {
        "id": "GmGfdMZb28ft"
      },
      "outputs": [],
      "source": [
        "# Data transformation defined\n",
        "transform = transforms.Compose([\n",
        "    # you may add more transformations here...\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_nrTC7303Sm4",
      "metadata": {
        "id": "_nrTC7303Sm4"
      },
      "source": [
        "### Load the CIFAR-10 dataset (~ 5 min)\n",
        "In the following code snippet, you'll see the initial setup for loading the CIFAR-10 dataset. Your task is setting up the data transformation, splitting the dataset into training and validation sets, and creating data loaders:\n",
        "\n",
        "- Calculate the size for the training set and validation set with len()\n",
        "- Split the training dataset into train and validation sets using random_split from torch.utils.data\n",
        "- Create data loaders for the training, validation, and test sets using DataLoader from torch.utils.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6srMVMyv042W",
      "metadata": {
        "id": "6srMVMyv042W"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# ToDo: Calculate the size for the training set and validation set with len(), with the split 80/20\n",
        "# Insert your own code here.\n",
        "\n",
        "# ToDo: Split the training dataset into train and validation sets using random_split from torch.utils.data\n",
        "# Insert your own code here.\n",
        "\n",
        "# ToDo: Create data loaders for the training, validation, and test sets using DataLoader from torch.utils.data\n",
        "# Insert your own code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QowF1BN84NaQ",
      "metadata": {
        "id": "QowF1BN84NaQ"
      },
      "source": [
        "### Plot the dataloader data (1 min)\n",
        "Now the `train_loader`, `val_loader`, and `test_loader` are ready to be used for training and evaluation.\n",
        "\n",
        "Lets plot some images: its a good practice to view the data before using it as input to the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D0pmHzDxFFG5",
      "metadata": {
        "id": "D0pmHzDxFFG5"
      },
      "outputs": [],
      "source": [
        "def plot_batch_images(data_loader, grid_size=(5, 5)):\n",
        "    # Get a random batch of data\n",
        "    batch_images, _ = next(iter(data_loader))\n",
        "\n",
        "    # Denormalize the images\n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std = np.array([0.5, 0.5, 0.5])\n",
        "    denormalized_images = batch_images.cpu().numpy() * std[:, None, None] + mean[:, None, None]\n",
        "    denormalized_images = np.clip(denormalized_images, 0, 1)  # Clip values to [0, 1]\n",
        "\n",
        "    # Create a grid of images\n",
        "    num_rows, num_cols = grid_size\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(3, 3))\n",
        "\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            image = denormalized_images[i * num_cols + j].transpose(1, 2, 0)\n",
        "            axes[i, j].imshow(image)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot random images from the train_loader in a 5x5 grid\n",
        "plot_batch_images(train_loader, (4,4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k8hKHvaoDUzT",
      "metadata": {
        "id": "k8hKHvaoDUzT"
      },
      "source": [
        "### Define the convolutional neural network architecture (5 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vyKze0wBFGlJ",
      "metadata": {
        "id": "vyKze0wBFGlJ"
      },
      "source": [
        "Below, you'll find the initial code to define the neural network model. Your task is to complete the Net class by adding the missing layers and connections as described in the comments. You'll be implementing a simple convolutional neural network (CNN) architecture.\n",
        "\n",
        "Your Task:\n",
        "Fill in the missing portions of the code in the cell below to complete the neural network model definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8tgkqt1y5BvP",
      "metadata": {
        "id": "8tgkqt1y5BvP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# Define the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # ToDo:\n",
        "        # 1. Add another convolutional layer with 128 output channels, a kernel size of 3 and padding of 1\n",
        "        # 2. Define two nn.fc1 and nn.fc2 fully connected (linear) layers: one with 128 output features and another with 10 output features\n",
        "\n",
        "        # Insert your own code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # ToDo:\n",
        "        # 3. Add a forward pass for the second convolutional layer\n",
        "        # 4. Reshape the tensor before passing it through the fully connected layers\n",
        "\n",
        "        # Insert your own code here\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the Net class\n",
        "model = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36sSDaKA7h1p",
      "metadata": {
        "id": "36sSDaKA7h1p"
      },
      "source": [
        "#### Define optimizer and loss function (5 min)\n",
        "Below, you'll find a code snippet to define the optimizer and loss function. Your task is to complete the code by filling in the appropriate optimizer and loss function classes along with the required hyperparameters.\n",
        "\n",
        "#### Your Task:\n",
        "Fill in the missing portions of the code in the cell below to set up the optimizer and loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dt5z0-h7it6",
      "metadata": {
        "id": "2dt5z0-h7it6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define optimizer and loss function\n",
        "# ToDo:\n",
        "# 1. Create an instance of the Adam optimizer and pass the model parameters and learning rate (lr=0.001)\n",
        "# 2. Create an instance of the CrossEntropyLoss loss function\n",
        "\n",
        "# Insert your own code here.\n",
        "\n",
        "# Uncomment the following lines to print the optimizer and loss function details\n",
        "# print(\"Optimizer:\", optimizer)\n",
        "# print(\"Loss Function:\", criterion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8KE4Thz58i8V",
      "metadata": {
        "id": "8KE4Thz58i8V"
      },
      "source": [
        "### Train the model (10-15 min)\n",
        "Below, you'll find a code snippet with placeholders for the training and validation loops. Your task is to complete the code by filling in the missing portions to implement the training and validation process.\n",
        "\n",
        "#### Your Task:\n",
        "Fill in the missing portions of the code in the cell below to complete the training and validation loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-1UuWd6i8f3W",
      "metadata": {
        "id": "-1UuWd6i8f3W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # ToDo:\n",
        "        # 1. Compute the loss using the criterion (loss function)\n",
        "        # 2. Perform backpropagation to compute gradients .backward()\n",
        "        # 3. Update model parameters using the optimizer .step()\n",
        "\n",
        "        # Insert your own code here\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # Your task:\n",
        "            # 4. Compute validation loss and accumulate it\n",
        "            # 5. Calculate the number of correct predictions and accumulate it\n",
        "\n",
        "            # Insert your own code here\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {correct/len(val_loader.dataset):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vzFSA8gm_mRX",
      "metadata": {
        "id": "vzFSA8gm_mRX"
      },
      "source": [
        "### Test the model (1 min)\n",
        "Below, you'll find a code snippet to test the trained model on the test set. Your task is to complete the code by filling in the missing portions to calculate and print the test loss and accuracy.\n",
        "\n",
        "#### Your Task:\n",
        "Fill in the missing portions of the code in the cell below to complete the testing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-RDCaW4EdwH",
      "metadata": {
        "id": "d-RDCaW4EdwH"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # Your task:\n",
        "        # 1. Compute the test loss using the criterion (loss function)\n",
        "        # 2. Calculate the number of correct predictions and accumulate it\n",
        "\n",
        "        # Insert your own code here\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {correct/len(test_loader.dataset):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zec8a-1rGfU8",
      "metadata": {
        "id": "zec8a-1rGfU8"
      },
      "source": [
        "Below, you'll find a code snippet to redefine the data transformations and adjust the batch size. Your task is to complete the code by replacing the existing transformation setup with the enhanced transformations and updating the batch size.\n",
        "\n",
        "#### Your Task (15 min):\n",
        "Replace the existing data transformation setup with the provided enhanced transformations and adjust the batch size to complete the code in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n9oE9riXGgHj",
      "metadata": {
        "id": "n9oE9riXGgHj"
      },
      "outputs": [],
      "source": [
        "# Import the CIFAR dataset\n",
        "transform_train = transforms.Compose([\n",
        "    # ToDo:\n",
        "    # 1. Add a RandomCrop transformation with padding of 4\n",
        "    # 2. Add a RandomHorizontalFlip transformation\n",
        "    # 3. Transform the image to a tensor and normalize it\n",
        "\n",
        "    # Insert your own code here\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([ # Test set normally remains untouched from sophisticated transformations\n",
        "    # ToDo: Transform the image to a tensor and normalize it\n",
        "    # Insert your own code here\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test, download=True)\n",
        "\n",
        "# ToDo: Split into training and validation\n",
        "train_size = #Insert your own code here\n",
        "val_size = #Insert your own code here\n",
        "train_dataset, val_dataset = #Insert your own code here\n",
        "\n",
        "# ToDo: Adjust the batch size\n",
        "batch_size = #Insert your own code here\n",
        "train_loader =#Insert your own code here\n",
        "val_loader = #Insert your own code here\n",
        "test_loader = #Insert your own code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYuW4h8oHpUx",
      "metadata": {
        "id": "EYuW4h8oHpUx"
      },
      "source": [
        "Below, you'll find a code snippet to load the pretrained [ResNet-18](https://arxiv.org/pdf/1512.03385.pdf) model and set up the training components. Your task is to complete the code by adding the missing parts to define the optimizer, loss function, and modify the fully connected layer for 10 classes.\n",
        "\n",
        "#### Your Task (10-15 min):\n",
        "Complete the code in the cell below by defining the optimizer and loss function, as well as modifying the fully connected layer of the pretrained ResNet-18 model to match the 10 classes of the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VPtmhW-bHpZZ",
      "metadata": {
        "id": "VPtmhW-bHpZZ"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "# ToDo: Inspect the output of the resnet with the model summary\n",
        "\n",
        "# ToDo: Modify the fully connected layer to output 10 classes nn.Linear()\n",
        "# Insert your own code here\n",
        "\n",
        "# Define optimizer and loss function\n",
        "# ToDo:\n",
        "# 2. Create an instance of the Adam optimizer and pass the model parameters and learning rate (lr=0.001)\n",
        "# 3. Create an instance of the CrossEntropyLoss loss function or you may use another\n",
        "\n",
        "# Insert your own code here\n",
        "\n",
        "# Send model to gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2NiCjYv9KQ0g",
      "metadata": {
        "id": "2NiCjYv9KQ0g"
      },
      "source": [
        "Here we'll enhance the training process by incorporating logging of training and validation metrics. Additionally, we'll integrate the tqdm library to provide a progress bar that gives feedback on the training process. Additionally, we will create informative plots that illustrate the model's performance during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XBtBVlFlKQ_z",
      "metadata": {
        "id": "XBtBVlFlKQ_z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define lists to store metrics\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss_total = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_total += loss.item()\n",
        "\n",
        "        _, predicted = output.max(1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    train_loss_avg = train_loss_total / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss_total = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "            _, predicted = output.max(1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss_avg = val_loss_total / len(val_loader)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    train_losses.append(train_loss_avg)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss_avg)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss_avg:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss_avg:.4f}, Validation Accuracy: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yMh3giVUPRtS",
      "metadata": {
        "id": "yMh3giVUPRtS"
      },
      "source": [
        "Here's how we can to plot the metrics stored before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nesg4X4ATOt-",
      "metadata": {
        "id": "Nesg4X4ATOt-"
      },
      "outputs": [],
      "source": [
        "def plot_loss_and_accuracy(epochs_range, train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plotting loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Train')\n",
        "    plt.plot(epochs_range, val_losses, label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_accuracies, label='Train')\n",
        "    plt.plot(epochs_range, val_accuracies, label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "plot_loss_and_accuracy(range(num_epochs), train_losses, val_losses, train_accuracies, val_accuracies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pIxDf_2kLP63",
      "metadata": {
        "id": "pIxDf_2kLP63"
      },
      "source": [
        "Here we'll enhance the previous test process by integrating the tqdm library to provide a progress bar that gives feedback on the testing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H5eof9mOLD17",
      "metadata": {
        "id": "H5eof9mOLD17"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "model.eval()\n",
        "test_loss_total = 0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in tqdm(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss_total += loss.item()\n",
        "\n",
        "        _, predicted = output.max(1)\n",
        "        test_total += target.size(0)\n",
        "        test_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / test_total\n",
        "test_loss_avg = test_loss_total / len(test_loader)\n",
        "\n",
        "print(f'Test Loss: {test_loss_avg:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5a94a7",
      "metadata": {
        "id": "2b5a94a7"
      },
      "source": [
        "### Visualise test (5 mins)\n",
        "Let us  visualise the results (similar to how we did for the MNIST dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50a2375",
      "metadata": {
        "id": "f50a2375"
      },
      "outputs": [],
      "source": [
        "#ToDo: visualise 20 images with titles containing the predicted and actual class. Print in green if they are the same, red otherwise.\n",
        "# Obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Get sample outputs\n",
        "output = model(images)\n",
        "_, preds = torch.max(output, 1)\n",
        "images = images.numpy()\n",
        "images = images.transpose(0, 2, 3, 1)\n",
        "\n",
        "# Plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx] == labels[idx] else \"red\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HlTy7olKRWTy",
      "metadata": {
        "id": "HlTy7olKRWTy"
      },
      "source": [
        "### Save your trained model\n",
        "\n",
        "Here's how you can save and load the trained model using PyTorch's torch.save() and torch.load() functions:\n",
        "\n",
        "Saving the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agKSjp4SRbLl",
      "metadata": {
        "id": "agKSjp4SRbLl"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ohBR2xw_RwN4",
      "metadata": {
        "id": "ohBR2xw_RwN4"
      },
      "source": [
        "To load the model later:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zj6aPXkSRcrv",
      "metadata": {
        "id": "zj6aPXkSRcrv"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 10)  # Change the fully connected layer for 10 classes\n",
        "\n",
        "model.load_state_dict(torch.load('model.pth'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "__707ZORQste",
      "metadata": {
        "id": "__707ZORQste"
      },
      "source": [
        "### Going the Extra Mile: Fine-Tuning a Pretrained Model (5 mins)\n",
        "\n",
        "In this advanced exercise, you'll take your training process to the next level by performing fine-tuning on a pre-trained model. Fine-tuning involves adjusting the weights of the pre-trained model to better fit the specific dataset you're working with. This approach can lead to improved performance on your target task.\n",
        "\n",
        "Below, you'll find the code snippet that demonstrates the process of fine-tuning a pre-trained model on the CIFAR-10 dataset. Your task is to run this code to fine-tune the model and observe any improvements in training and validation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UiuLJjphSdis",
      "metadata": {
        "id": "UiuLJjphSdis"
      },
      "source": [
        "First we freeze the last # layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5yV_3G05QsS5",
      "metadata": {
        "id": "5yV_3G05QsS5"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers except the last # layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SH0fmfkiSiv_",
      "metadata": {
        "id": "SH0fmfkiSiv_"
      },
      "source": [
        "Then we use a smaller learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N6vnVo-NSlGn",
      "metadata": {
        "id": "N6vnVo-NSlGn"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y9kmoA4LSpe7",
      "metadata": {
        "id": "y9kmoA4LSpe7"
      },
      "source": [
        "Now we train again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad37d25",
      "metadata": {
        "id": "6ad37d25"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define lists to store metrics\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss_total = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_total += loss.item()\n",
        "\n",
        "        _, predicted = output.max(1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    train_loss_avg = train_loss_total / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss_total = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(val_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "            _, predicted = output.max(1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss_avg = val_loss_total / len(val_loader)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    train_losses.append(train_loss_avg)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss_avg)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss_avg:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss_avg:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Usage\n",
        "plot_loss_and_accuracy(epochs_range, train_losses, val_losses, train_accuracies, val_accuracies)\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "test_loss_total = 0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in tqdm(test_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        test_loss_total += loss.item()\n",
        "\n",
        "        _, predicted = output.max(1)\n",
        "        test_total += target.size(0)\n",
        "        test_correct += predicted.eq(target).sum().item()\n",
        "\n",
        "test_accuracy = test_correct / test_total\n",
        "test_loss_avg = test_loss_total / len(test_loader)\n",
        "\n",
        "print(f'Test Loss: {test_loss_avg:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad3cdc4",
      "metadata": {
        "id": "5ad3cdc4"
      },
      "outputs": [],
      "source": [
        "#ToDo: visualise 20 images with titles containing the predicted and actual class. Print in green if they are the same, red otherwise.\n",
        "# Obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Get sample outputs\n",
        "output = model(images)\n",
        "_, preds = torch.max(output, 1)\n",
        "images = images.numpy()\n",
        "images = images.transpose(0, 2, 3, 1)\n",
        "\n",
        "# Plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx] == labels[idx] else \"red\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GWdZb6l7mz7f",
      "metadata": {
        "id": "GWdZb6l7mz7f"
      },
      "source": [
        "In this comprehensive exercise, you've worked through a complete deep learning workflow using the CIFAR-10 dataset. You've covered data preprocessing, model definition, training, validation, testing, and advanced techniques like fine-tuning and visualizing performance metrics. This exercise aimed to provide you with a hands-on experience in building and training neural networks using PyTorch."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}