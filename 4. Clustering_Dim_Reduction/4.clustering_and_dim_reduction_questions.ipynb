{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SAAI** Overview | Background\n",
        "\n",
        "The Summer School on Affordable AI **SAAI** is a project of the AGYA working group Innovation in close collaboration with the AGYA working group Health and Society. [The Arab-German Young Academy of Sciences and Humanities (AGYA)](https://agya.info/) is funded by the  [German Federal Ministry of Education and Research (BMBF)](https://www.bmbf.de/bmbf/en/home/home_node.html) and various Arab and German cooperation partners.\n",
        "<img src=\"https://imgur.com/hMpk6HK.png\" width=\"800\">\n",
        "<img src=\"https://imgur.com/b9n0Ow7.jpg\" width=\"800\">\n",
        "<img src=\"https://imgur.com/EvXNz7j.jpg\" width=\"800\">\n",
        "<img src=\"https://imgur.com/amMP7c7.jpg\" width=\"800\">"
      ],
      "metadata": {
        "id": "RUhwqk7-ckqf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwXTJxCn2Tsr"
      },
      "source": [
        "# **Clustering and Dimension Reduction (~ 75 min)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSD7oNgr_Tb9"
      },
      "source": [
        "<img src=\"https://imgur.com/SoVQEM3.png\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp3veUjGxHcU"
      },
      "source": [
        "In this notebook we will study and work with the sk-learn Wine dataset, a classic dataset often used in machine learning for classification tasks. The Wine dataset consists of 178 samples, each characterised by 13 distinct features representing different chemical analyses of Italian wines from three different varieties. These features include a range of chemical properties such as alcohol content, malic acid, ash, alkalinity of ash, magnesium, total phenols, flavonoids, non-flavonoid phenols, proanthocyanins, colour intensity, hue, OD280/OD315 of diluted wines and proline. The primary aim of this notebook is to go through the clustering and dimensionality reduction techniques.\n",
        "\n",
        "\n",
        "Our first objective will be to learn how to effectively cluster the samples in this dataset using the K-means function. This technique will involve grouping the wine samples based on their chemical properties, with the aim of identifying inherent structures within the data. Following the clustering process, we will turn our attention to evaluating the performance of our clustering function. Finally, we will explore three different dimensionality reduction techniques. These techniques are instrumental in simplifying the complexity of our dataset while retaining as much of the significant information as possible. By applying these methods, we aim to obtain a clearer, more accessible low-dimensional space visualisation of the features within the Wine dataset. This step is essential to better understand the relationships between different variables and to make our dataset more manageable for further analysis and interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI61KwSNBKtI"
      },
      "source": [
        "## Import the necessary libraries and load the data (5 min):\n",
        "\n",
        "1. The umap library is not pre-installed on your notebook. Install it with `!pip'.\n",
        "\n",
        "2. Familiarise yourself with the required libraries and use an abbreviation to refer to them.\n",
        "\n",
        "3. Load the dataset using the `load_wine()` function. Determine the number of different classes that are present. To get an initial overview, use `df.columns' and `pd.head()` to display the feature names and the first 5 records of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nikvLYSbmM9C"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_EPtE-5-4N1",
        "outputId": "fffdd5b6-c9fa-44f5-d76a-639e22ad8559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from umap-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from umap-learn) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from umap-learn) (0.24.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "Collecting numba>=0.51.2\n",
            "  Downloading numba-0.56.4-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0\n",
            "  Downloading llvmlite-0.39.1-cp37-cp37m-win_amd64.whl (23.2 MB)\n",
            "Requirement already satisfied: importlib-metadata in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from numba>=0.51.2->umap-learn) (3.4.0)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\python37\\lib\\site-packages (from numba>=0.51.2->umap-learn) (47.1.0)\n",
            "Collecting importlib-metadata\n",
            "  Using cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from pynndescent>=0.5->umap-learn) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata->numba>=0.51.2->umap-learn) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata->numba>=0.51.2->umap-learn) (3.7.4.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\renar\\appdata\\roaming\\python\\python37\\site-packages (from tqdm->umap-learn) (0.4.4)\n",
            "Installing collected packages: llvmlite, importlib-metadata, numba, tqdm, pynndescent, umap-learn\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 3.4.0\n",
            "    Uninstalling importlib-metadata-3.4.0:\n",
            "      Successfully uninstalled importlib-metadata-3.4.0\n",
            "Successfully installed importlib-metadata-6.7.0 llvmlite-0.39.1 numba-0.56.4 pynndescent-0.5.13 tqdm-4.66.4 umap-learn-0.5.6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.1.2; however, version 24.0 is available.\n",
            "You should consider upgrading via the 'c:\\program files\\python37\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2baBHnekYGJQ",
        "outputId": "1199a138-9213-44b1-da9e-80fd99096ca0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'plotly'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-c88f96e4c61f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_wine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2pxfXmMYVYX"
      },
      "outputs": [],
      "source": [
        "df = # ... Your code goes here\n",
        "df = df.frame\n",
        "# print the shape of the dataset, the list of columns name, and the first 5 rows of dataset\n",
        "print(df.shape)\n",
        "print(list(df.columns))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrxKF9BLD0Vv"
      },
      "source": [
        "## Data Standardization (5 min):\n",
        "\n",
        "In data science, \"standardization\" and \"normalization\" are two common techniques used to prepare data for machine learning algorithms. These techniques are critical for models that are sensitive to the scale and distribution of the data.\n",
        "\n",
        "Standardization refers to transforming each feature to have a mean of zero and a standard deviation of one. This process is also known as z-score normalization. The formula for standardizing a characteristic is: $z = \\frac{x - \\mu}{\\sigma}$.\n",
        "\n",
        "Standardization does not bind values to a specific range. It's useful when you want to compare features that have different units or scales. It is essential for algorithms that assume the data is normally distributed, and for methods that are sensitive to the scale of the input features (e.g., PCA, SVM).\n",
        "\n",
        "Normalization, on the other hand, typically refers to scaling individual samples so that they have unit norm length. It's more about changing the scale of the data without changing its distribution. The most commonly used norms are the L1 norm (also known as the Manhattan distance) and the L2 norm (Euclidean distance): $x_{\\text{norm}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$.\n",
        "\n",
        "\n",
        "\n",
        "Read more here:\n",
        "\n",
        "https://developers.google.com/machine-learning/data-prep/transform/normalization\n",
        "\n",
        "The `StandardScaler().fit_transform(df)` method from the scikit-learn library ensures that each feature contributes equally to the analysis. This method standardizes the features by removing the mean and scaling to unit variance, which is crucial for algorithms like K-means that are sensitive to the scale of the data. By applying this transformation, it helps improve the performance and accuracy of machine learning models, as they are not biased or skewed by the natural variance in the dataset.\n",
        "\n",
        "1. Standardize the data using the sk-learn functions `StandardScaler().fit_transform(df)`.\n",
        "\n",
        "2. Print the first 5 rows of data and compare them with the data before standardization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ECJ6uPfxJR"
      },
      "outputs": [],
      "source": [
        "df_features = df.drop(['target'], axis=1)\n",
        "\n",
        "scaler = # ... Your code goes here\n",
        "features = # ... Your code goes here\n",
        "features = # ... Your code goes here\n",
        "# 3 lines of code in 1 line:\n",
        "# features = StandardScaler().fit_transform(df_features)\n",
        "\n",
        "# Convert to pandas Dataframe\n",
        "# you may want to repeat the visualizaton again this time without the standardization\n",
        "scaled_df = # ... Your code goes here)  # without\n",
        "scaled_df = # ... Your code goes here     # with\n",
        "\n",
        "# Print the scaled data\n",
        "scaled_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaTMP4x1EbbL"
      },
      "source": [
        "## K-mean and Elbow Method (10 min)\n",
        "\n",
        "Plotting the elbow curve for K-means clustering is essential for determining the optimal number of clusters (k) for the data set. The curve, which plots the number of clusters against a measure of cluster quality (such as WCSS: Within-Cluster Sum of Squares, or NMI: Normalized Mutual Information) typically shows a point where the rate of decrease changes sharply, known as the \"elbow\". This point indicates where adding more clusters does not significantly improve the clustering quality, thus providing a balance between the complexity of the model and the adequacy of the cluster representation.\n",
        "\n",
        "\n",
        "WCSS (Within-Cluster Sum of Squares) and NMI (Normalized Mutual Information) are two metrics commonly used in clustering, a form of unsupervised learning in data science. Let's take a closer look at what each of these metrics means:\n",
        "\n",
        "> WCSS (Within-Cluster Sum of Squares)\n",
        "WCSS is a metric used to evaluate the performance of a clustering algorithm, most commonly k-means. It measures the sum of the squared distances between each point in a cluster and the centroid of that cluster. The formula for WCSS is $\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2$\n",
        "\n",
        "$k$ is the number of clusters. $C_i$ is the set of points in the $i$th cluster. $x$ is a point in cluster $C_i$. $\\mu_i$ is the centroid of cluster $C_i$. $\\| x - \\mu_i \\|^2$ is the squared Euclidean distance between point $x$ and centroid $\\mu_i$.\n",
        "\n",
        "The goal in k-means is to minimize the WCSS, as a lower WCSS indicates that the points within each cluster are closer to their respective centroids, implying better compactness and separation of the clusters.\n",
        "\n",
        "> NMI (Normalized Mutual Information)\n",
        "NMI is a measure used to evaluate the quality of clustering, especially in scenarios where the true labels are known. It is based on mutual information (MI), which measures the amount of information about a random variable that can be obtained by observing another random variable. NMI normalizes this value to account for the size of the clusters and the distribution of the ground truth labels. The formula for NMI is $\\text{NMI}(U, V) = \\frac{2 \\times \\text{I}(U; V)}{H(U) + H(V)}$.\n",
        "\n",
        "$U$ is the set of true class labels. $V$ is the set of cluster labels generated by the clustering algorithm. $\\text{I}(U; V)$ is the mutual information between $U$ and $V$. $H(U)$ and $H(V)$ are the entropies of $U$ and $V$, respectively.\n",
        "\n",
        "Read more here:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html\n",
        "\n",
        "\n",
        "NMI values range from 0 to 1, where 1 indicates perfect clustering (i.e., the clustering algorithm perfectly predicts the true labels) and 0 indicates that the clustering is no better than random chance. The NMI is particularly useful for comparing the performance of different clustering algorithms, or the same algorithm with different parameters. Follow the steps below to complete the evaluation:\n",
        "\n",
        "\n",
        "\n",
        "1. For a number of clusters ranging from 1 to 10, plot the elbow curve for kmeans clustering and select a final number of clusters.\n",
        "2. Evaluate your kmeans with `kmeans.inertia_`. Create WCSS (Within-Cluster Sum of Squares): a dictionary to store the values for each num of clusters k\n",
        "3. Use NMI `normalized_mutual_info_score()` to re-evaluate your k-means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jnwaN-u2-edd"
      },
      "outputs": [],
      "source": [
        "# for num of clusters 1 to 10, plot the elbow curve\n",
        "X = scaled_df.values\n",
        "# WCSS (Within-Cluster Sum of Squares): a dictionary to store the values for each num of clusters k\n",
        "wcss = {}\n",
        "for i in range(1, 11):\n",
        "    kmeans = # ... Your code goes here\n",
        "    kmeans.fit(X)\n",
        "    wcss[i] = # ... Your code goes here\n",
        "\n",
        "\n",
        "plt.plot(wcss.keys(), wcss.values(), 'gs-')\n",
        "plt.xlabel(\"Values of 'k'\")\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()\n",
        "\n",
        "# according to the plot, choose the suitable number of clusters and fit the model\n",
        "kmeans = # ... Your code goes here\n",
        "kmeans.fit(X)\n",
        "print(kmeans.cluster_centers_.shape) # (3, 13)\n",
        "print(kmeans.cluster_centers_) # 3 clusters, 13 features\n",
        "print(kmeans.labels_) # 178 samples, each sample belongs to one of the 3 clusters\n",
        "\n",
        "# Evaluate your Kmeans\n",
        "# get assigned labels and for input to the kmeans algorithm\n",
        "labels = # ... Your code goes here\n",
        "true_labels = # ... Your code goes here\n",
        "kmeans_labels = # ... Your code goes here\n",
        "nmi_score = normalized_mutual_info_score() # ... Your code goes here)\n",
        "print(\"NMI Score: \", nmi_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vz0krEmGjUN"
      },
      "source": [
        "## Dimensionality Reduction (10 min)\n",
        "\n",
        "\n",
        "Dimensionality reduction of data features is a critical process in data analysis and machine learning, especially when dealing with high-dimensional datasets. Learn why it's important and the differences between PCA, t-SNE, and UMAP:\n",
        "Importance of dimensionality reduction:\n",
        "- Reduces computational complexity: By reducing the number of features, it makes algorithms faster and less computationally expensive.\n",
        "- Mitigates the Curse of Dimensionality: High-dimensional spaces often lead to problems such as overfitting and sparsity of data; reducing dimensions can alleviate these problems.\n",
        "- Finally, it improves data visualization: It's easier to visualize and interpret data in two or three dimensions than in higher-dimensional spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gft5wFxeM0Wy"
      },
      "source": [
        "### PCA (Principal Component Analysis):\n",
        "\n",
        "Linear technique: PCA is a linear algorithm that identifies the directions (principal components) in which the data varies the most.\n",
        "Global structure preservation: It's effective at preserving the global structure of the data, but may not capture complex, nonlinear relationships.\n",
        "Scalability and interpretability: PCA scales well with large datasets, and the components can often be interpreted in terms of the original features.\n",
        "\n",
        "Click here to learn more:\n",
        "https://www.youtube.com/watch?v=HMOI_lkzW08\n",
        "\n",
        "\n",
        "Follow the steps below to complete the tasks:\n",
        "\n",
        "1. Reduce the dimension of the features to 2 using the imported PCA function `PCA()`.\n",
        "2. Visualize the samples in 2D along with the centroids generated by Kmean with 3 clusters. Remember that you must also reduce the features of the centroids to visualize them in 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vO7Bvw5S-hT2"
      },
      "outputs": [],
      "source": [
        "pca = # ... Your code goes here\n",
        "reduced_X = pd.DataFrame(...) # ... Your code goes here\n",
        "print(reduced_X.head())\n",
        "# reduced centers features after PCA\n",
        "centers = pca.transform(...) # ... Your code goes here\n",
        "centers\n",
        "\n",
        "\n",
        "# Scatter plot of the reduced data\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.scatter(reduced_X['PCA1'],reduced_X['PCA2'],c=kmeans.labels_)\n",
        "plt.scatter(centers[:,0],centers[:,1],marker='x',s=100,c='red')\n",
        "plt.xlabel('dim1_after_PCA')\n",
        "plt.ylabel('dim2_after_PCA')\n",
        "plt.title('Wine Clusters')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_Xkqih7B-p"
      },
      "source": [
        "#### Transforming the Features of a Sample into a Low-Dimensional Space: (15 min)\n",
        "\n",
        "The goal of this exercise is to familiarize you with the process of transforming a high-dimensional data sample into a lower-dimensional space using PCA. You will work with a sample containing 13 features and project it into a 2D space.\n",
        "\n",
        "\n",
        "\n",
        "To project a sample from a 13-dimensional space to a 2-dimensional space using a transformation matrix, you typically use a method such as Principal Component Analysis (PCA). Here's a simplified overview of the math behind this process:\n",
        "\n",
        "1. **PCA Transformation Matrix:**\n",
        "   - PCA begins by calculating the eigenvectors and eigenvalues from the covariance matrix of the data. These eigenvectors are the principal components.\n",
        "   - In this case, to project down to 2D, you select the first two principal components.\n",
        "   - Let's say your PCA transformation matrix $T$ consists of these two principal components, each of them being a 13-dimensional vector. Thus, $T$ will be a matrix of size $2 \\times 13$.\n",
        "\n",
        "2. **Sample in 13-dimensional space:**.\n",
        "   - Suppose your sample in 13-dimensional space is a vector $X$ of size $13 \\times$.\n",
        "\n",
        "3. **Projection to 2D:**.\n",
        "   - The projection of $X$ into the new 2D space is obtained by multiplying the transformation matrix $T$ with the sample vector $X$.\n",
        "   - Mathematically, it's represented as $Y = TX$ , where $Y$ is your new 2D representation of the sample.\n",
        "   - In terms of matrix dimensions, it looks like $(2 \\times 13) \\times (13 \\times 1) = (2 \\times 1)$. Thus, $Y$ is a 2-dimensional vector.\n",
        "\n",
        "4. **Matrix Multiplication:**.\n",
        "   - The actual calculation is the dot product between each row of $T$ and $X$. Each element of $Y$ is a sum of the products of the corresponding elements from the row of $T$ and $X$.\n",
        "\n",
        "5. **Result:**\n",
        "   - The resulting 2D vector $Y$ represents your original 13-dimensional sample projected into the space defined by the first two principal components. This 2D representation captures the most significant variance of the original data in two dimensions.\n",
        "\n",
        "This process effectively reduces the dimensionality of the data while retaining the most important information as captured by the first two principal components. Now follow the steps below for implementation details:\n",
        "\n",
        "1. Select a random sample from the data set.\n",
        "2. Use `pca.transform` to transform the 13 features of this sample to 2D.\n",
        "3. Plot a heatmap of the transform matrix `pca.components`.  \n",
        "4. Using the transform matrix `pca.components` and the dot product function `np.dot` try to find the sample projection in 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qcL5T-6d1_h0"
      },
      "outputs": [],
      "source": [
        "# Heat map c\n",
        "print(pca.components_.shape)\n",
        "component_df = pd.DataFrame() # ... Your code goes here\n",
        "sns.heatmap(component_df)\n",
        "plt.show()\n",
        "\n",
        "# use the PCA transfer matrix to transform the sample to low-dim space\n",
        "sample = # ... Your code goes here\n",
        "pca_transfer_Matrix = # ... Your code goes here\n",
        "sample_2component = np.dot() # ... Your code goes here\n",
        "# now use the PCA funciton\n",
        "sample_transformed = # ... Your code goes here\n",
        "\n",
        "print('----')\n",
        "print('transfer with function  : ', sample_2component)\n",
        "print('transfer with dotProduct: ', sample_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2PoUZgkIBkU"
      },
      "source": [
        "#### Dimensionality Reduction with PCA: (5 min)\n",
        "1. Repeat the dimensionality reduction, this time to 3. Again using the imported PCA function `PCA()`.\n",
        "2. Visualize the 3D components of the samples.\n",
        "3. Also use the interactive visualization library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ZmUQVz_-ld0"
      },
      "outputs": [],
      "source": [
        "# Perform PCA for visualization with 3 components\n",
        "pca = # ... Your code goes here\n",
        "X_pca = # ... Your code goes here\n",
        "\n",
        "# Plot the first three principal components in 3D\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# in the scatter plot bellow, GIVE EACH SAMPLE A COLOR ACCORDING TO ITS TARGET VALUE\n",
        "scatter = ax.scatter() # ... Your code goes here\n",
        "\n",
        "\n",
        "plt.colorbar(scatter, label='Digit')\n",
        "ax.set_title('PCA Visualization of Wine Data (3D)')\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "plt.show()\n",
        "\n",
        "fig = px.scatter_3d() # ... Your code goes here\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrrH5IXcIipO"
      },
      "source": [
        "### t-SNE : t-Distributed Stochastic Neighbor Embedding (7 min):\n",
        "\n",
        "Nonlinear Technique: t-SNE is a nonlinear, probabilistic technique used primarily for visualizing high-dimensional data in 2D or 3D.\n",
        "Local structure preservation: It excels at revealing local structure and clustering in data, often at the expense of global structure.\n",
        "Computational complexity: t-SNE can be computationally intensive, especially on large datasets, and its results can vary depending on the parameters chosen.\n",
        "\n",
        "1. Reduce the dimension of the features to 2 using the imported `TSNE()` function.\n",
        "2. Visualize the samples in 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzgoeWEB-nk-"
      },
      "outputs": [],
      "source": [
        "# Perform t-SNE\n",
        "tsne = # ... Your code goes here\n",
        "X_tsne = # ... Your code goes here\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter() # ... Your code goes here\n",
        "plt.colorbar()\n",
        "plt.title('t-SNE Visualization of Wine Data')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsxPTBlBImcD"
      },
      "source": [
        "### UMAP : Uniform Manifold Approximation and Projection (8 min):\n",
        "\n",
        "Nonlinear technique: Like t-SNE, UMAP is a nonlinear method, but it's based on manifold learning with superior scalability. Balance between local and global structure: UMAP preserves more of the global structure than t-SNE, while also revealing local structures. Versatility and speed: It works faster than t-SNE on large datasets and can be applied more broadly, including in tasks beyond visualization, such as supervised dimensionality reduction.\n",
        "\n",
        "1. Reduce the dimension of the features to 2 with the imported function `umap.UMAP()`.\n",
        "2. Visualize the samples in 2D."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCDoSf_Q-pMU"
      },
      "outputs": [],
      "source": [
        "# Perform UMAP dimensionality reduction\n",
        "reducer = # ... Your code goes here\n",
        "embedding = # ... Your code goes here\n",
        "\n",
        "# Visualize the results\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter() # ... Your code goes here\n",
        "plt.colorbar()\n",
        "plt.title('UMAP Visualization of Wine Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FHODZuiNwu5"
      },
      "source": [
        "### PCA, t-SNE, Umap\n",
        "Each of these methods has its strengths and is suited to different types of datasets and analysis goals. PCA is often the first choice for linear dimensionality reduction and overview analysis, while t-SNE and UMAP are more suited to detailed exploratory analysis, especially when the data has complex, non-linear relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2QVUqa6b9DT"
      },
      "source": [
        "### Questionnaire\n",
        "Hope you enjoyed the exercise!\n",
        "Before you leave, please fill in our questionnaire (link via the qr code below).\n",
        "Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxZaVRChb9DT"
      },
      "source": [
        "<img src=\"https://github.com/albarqounilab/SAAI-Summer-School/raw/main/questionnaires/Clustering_question_qr.png\" width=\"200\"/>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}